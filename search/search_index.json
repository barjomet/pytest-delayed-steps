{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pytest-steps \u00b6 Create step-wise / incremental tests in pytest . Manual execution of tests has slightly changed in 1.7.0 , see explanations here New 'generator' style is there, check it out ! New pytest-harvest compatibility fixtures, check them out ! Did you ever want to organize your test in incremental steps, for example to improve readability in case of failure ? Or to have some optional steps, executing only conditionally to previous steps' results? pytest-steps leverages pytest and its great @pytest.mark.parametrize and @pytest.fixture decorators, so that you can create incremental tests with steps without having to think about the pytest fixture/parametrize pattern that has to be implemented for your particular case. This is particularly helpful if: you wish to share a state / intermediate results across test steps your tests already rely on other fixtures and/or parameters , such as with pytest-cases . In that case, finding the correct pytest design, that will ensure that you have a brand new state object at the beginning of each test suite while ensuring that this object will actually be shared across all steps, might be very tricky. With pytest-steps you don't have to care about the internals: it just works as expected. Note pytest-steps has not yet been tested with pytest-xdist. See #7 Installing \u00b6 > pip install pytest_steps 1. Usage - \"generator\" mode \u00b6 This new mode may seem more natural and readable to non-pytest experts. However it may be harder to debug when used in combination with other pytest tricks. In such case, do not hesitate to switch to good old \"explicit\" mode . a- Basics \u00b6 Start with you favorite test function. There are two things to do, to break it down into steps: decorate it with @test_steps to declare what are the steps that will be performed, as strings. insert as many yield statements in your function body as there are steps. The function should end with a yield (not return !). !!! note Code written after the last yield will not be executed. For example we define three steps: from pytest_steps import test_steps @test_steps ( 'step_a' , 'step_b' , 'step_c' ) def test_suite (): # Step A print ( \"step a\" ) assert not False # replace with your logic intermediate_a = 'hello' yield # Step B print ( \"step b\" ) assert not False # replace with your logic yield # Step C print ( \"step c\" ) new_text = intermediate_a + \" ... augmented\" print ( new_text ) assert len ( new_text ) == 56 yield That's it! If you run pytest you will now see 3 tests instead of one: ============================= test session starts ============================= (...) collected 3 items (...)/test_example.py::test_suite[step_a] <- <decorator-gen-3> PASSED [ 33%] (...)/test_example.py::test_suite[step_b] <- <decorator-gen-3> PASSED [ 66%] (...)/test_example.py::test_suite[step_c] <- <decorator-gen-3> PASSED [100%] ========================== 3 passed in 0.06 seconds =========================== Debugging note You might wish to use yield <step_name> instead of yield at the end of each step when debugging if you think that there is an issue with the execution order. This will activate a built-in checker, that will check that each step name in the declared sequence corresponds to what you actually yield at the end of that step. b- Shared data \u00b6 By design, all intermediate results created during function execution are shared between steps, since they are part of the same python function call. You therefore have nothing to do: this is what is shown above in step c where we reuse intermediate_a from step a. c- Optional steps and dependencies \u00b6 In this generator mode, all steps depend on all previous steps by default: if a step fails, all subsequent steps will be skipped. To circumvent this behaviour you can declare a step as optional. This means that subsequent steps will not depend on it except explicitly stated. For this you should: wrap the step into the special optional_step context manager, yield the corresponding context object at the end of the step, instead of None or the step name. This is very important, otherwise the step will be considered as successful by pytest! For example: # Step B with optional_step ( 'step_b' ) as step_b : print ( \"step b\" ) assert False yield step_b If steps depend on an optional step in order to execute, you should make them optional too, and state it explicitly: declare the dependency using the depends_on argument. use should_run() to test if the code block should be executed. The example below shows 4 steps, where steps a and d are mandatory and b and c are optional with c dependent on b: from pytest_steps import test_steps , optional_step @test_steps ( 'step_a' , 'step_b' , 'step_c' , 'step_d' ) def test_suite_opt (): # Step A assert not False yield # Step B with optional_step ( 'step_b' ) as step_b : assert False yield step_b # Step C depends on step B with optional_step ( 'step_c' , depends_on = step_b ) as step_c : if step_c . should_run (): assert True yield step_c # Step D assert not False yield Running it with pytest shows the desired behaviour: step b fails but does not prevent step d to execute correctly. step c is marked as skipped because its dependency (step b) failed. ============================= test session starts ============================= (...) collected 4 items (...)/test_example.py::test_suite_opt[step_a] <- <decorator-gen-3> PASSED [ 25%] (...)/test_example.py::test_suite_opt[step_b] <- <decorator-gen-3> FAILED [ 50%] (...)/test_example.py::test_suite_opt[step_c] <- <decorator-gen-3> SKIPPED [ 75%] (...)/test_example.py::test_suite_opt[step_d] <- <decorator-gen-3> PASSED [100%] ================================== FAILURES =================================== _______________ test_suite_optional_and_dependent_steps[step_b] _______________ (...) ================ 1 failed, 2 passed, 1 skipped in 0.16 seconds ================ d- Calling decorated functions manually \u00b6 In some cases you might wish to call your test functions manually before the tests actually run. This can be very useful when you do not wish the package import times to be counted in test execution durations - typically in a \"benchmarking\" use case such as shown here . It is now possible to call a test function decorated with @test_steps manually. For this the best way to understand what you have to provide is to inspect it. from pytest_steps import test_steps @test_steps ( 'first' , 'second' ) def test_dummy (): print ( 'hello' ) yield print ( 'world' ) yield print ( help ( test_dummy )) yields test_dummy(________step_name_, request) So we have to provide two arguments: ________step_name_ and request . Note: the same information can be obtained in a more formal way using signature from the inspect (latest python) or funcsigs (older) packages. Once you know what arguments you have to provide, there are two rules to follow in order to execute the function manually: replace the step_name argument with which steps you wish to execute: None to execute all steps in order, or a list of steps to execute some steps only. Note that in generator mode, \"by design\" (generator function) it is only possible to call the steps in correct order and starting from the first one, but you can provide a partial list: replace the request argument with None , to indicate that you are executing outside of any pytest context. > test_dummy ( None , None ) hello world > test_dummy ( 'first' , None ) hello > test_dummy ( 'second' , None ) ValueError : Incorrect sequence of steps provided for manual execution . Step #1 should be named 'first', found 'second' arguments order changed in 1.7.0 Unfortunately the order of arguments for manual execution changed in version 1.7.0 . This was the only way to add support for class methods . Apologies ! e- Compliance with pytest \u00b6 parameters \u00b6 Under the hood, the @test_steps decorator simply generates a wrapper function around your function and mark it with @pytest.mark.parametrize . The function wrapper is created using the excellent decorator library, so all marks that exist on it are kept in the process, as well as its name and signature. Therefore @test_steps should be compliant with all native pytest mechanisms. For exemple you can use decorators such as @pytest.mark.parametrize before or after it in the function decoration order (depending on your desired resulting test order): @test_steps ( 'step_a' , 'step_b' ) @pytest . mark . parametrize ( 'i' , range ( 2 ), ids = lambda i : \"i= %i \" % i ) def test_suite_p ( i ): # Step A print ( \"step a, i= %i \" % i ) assert not False # replace with your logic yield # Step B print ( \"step b, i= %i \" % i ) assert not False # replace with your logic yield If you execute it, it correctly executes all the steps for each parameter value: ============================= test session starts ============================= (...) collected 4 items (...)/test_example.py::test_suite_p[i=0-step_a] <- <decorator-gen-3> PASSED [ 25%] (...)/test_example.py::test_suite_p[i=0-step_b] <- <decorator-gen-3> PASSED [ 50%] (...)/test_example.py::test_suite_p[i=1-step_a] <- <decorator-gen-3> PASSED [ 75%] (...)/test_example.py::test_suite_p[i=1-step_b] <- <decorator-gen-3> PASSED [100%] ========================== 4 passed in 0.07 seconds =========================== fixtures \u00b6 You can also use fixtures as usual, but special care has to be taken about function-scope fixtures . Let's consider the following example: usage_counter = 0 @pytest . fixture def my_fixture (): \"\"\"Simple function-scoped fixture that return a new instance each time\"\"\" global usage_counter usage_counter += 1 print ( \"created my_fixture %s \" % usage_counter ) return usage_counter @test_steps ( 'step_a' , 'step_b' ) def test_suite_one_fixture_per_step ( my_fixture ): # Step A print ( \"step a\" ) assert my_fixture == 1 yield # Step B print ( \"step b\" ) assert my_fixture == 2 # >> raises an AssertionError because my_fixture = 1 ! yield Here, and that can be a bit misleading, pytest will call my_fixture() twice, because there are two pytest function executions, one for each step. So we think that everything is good... ...however the second fixture instance is never be passed to our test code: instead, the my_fixture instance that was passed as argument in the first step will be used by all steps. Therefore we end up having a failure in the test furing step b. It is possible to circumvent this behaviour by declaring explicitly what you expect: if you would like to share fixture instances across steps, decorate your fixture with @cross_steps_fixture . if you would like each step to have its own fixture instance, decorate your fixture with @one_fixture_per_step . For example from pytest_steps import one_fixture_per_step @pytest . fixture @one_fixture_per_step def my_fixture (): \"\"\"Simple function-scoped fixture that return a new instance each time\"\"\" global usage_counter usage_counter += 1 return usage_counter Each step will now use its own fixture instance and the test will succeed (instance 2 will be available at step b). When a fixture is decorated with @one_fixture_per_step , the object that is injected in your test function is a transparent proxy of the fixture, so it behaves exactly like the fixture. If for some reason you want to get the \"true\" inner wrapped object, you can do so using get_underlying_fixture(my_fixture) . 2. Usage - \"explicit\" mode \u00b6 In \"explicit\" mode, things are a bit more complex to write but can be easier to understand because it does not use generators, just simple function calls. a- Basics \u00b6 Like for the other mode, simply decorate your test function with @test_steps and declare what are the steps that will be performed. In addition, put a test_step parameter in your function, that will receive the current step. The easiest way to use it is to declare each step as a function: from pytest_steps import test_steps def step_a (): # perform this step ... print ( \"step a\" ) assert not False # replace with your logic def step_b (): # perform this step print ( \"step b\" ) assert not False # replace with your logic @test_steps ( step_a , step_b ) def test_suite_1 ( test_step ): # Optional: perform step-specific reasoning, for example to select arguments if test_step . __name__ == \"step_a\" : print ( \"calling step a\" ) # Execute the step by calling the test step function test_step () Note: as shown above, you can perform some reasoning about the step at hand in test_suite_1 , by looking at the test_step object. Custom parameter name You might want another name than test_step to receive the current step. The test_step_argname argument can be used to change that name. Variants: other types \u00b6 This mechanism is actually nothing more than a pytest parameter so it has to requirement on the test_step type. It is therefore possible to use other types, for example to declare the test steps as strings instead of function: @test_steps ( 'step_a' , 'step_b' ) def test_suite_2 ( test_step ): # Execute the step according to name if test_step == 'step_a' : step_a () elif test_step == 'step_b' : step_b () ... This has pros and cons: (+) you can declare the test suite before the step functions in the python file (better readability !) (-) you can not use @depends_on to decorate your step functions: you can only rely on shared data container to create dependencies (as explained below) b- Auto-skip/fail \u00b6 In this explicit mode all steps are optional/independent by default: each of them will be run, whatever the execution result of others. If you wish to change this, you can use the @depends_on decorator to mark a step as to be automatically skipped or failed if some other steps did not run successfully. For example: from pytest_steps import depends_on def step_a (): ... @depends_on ( step_a ) def step_b (): ... That way, step_b will now be skipped if step_a does not run successfully. Note that if you use shared data (see below), you can perform similar, and also more advanced dependency checks, by checking the contents of the shared data and calling pytest.skip() or pytest.fail() according to what is present. See step_b in the example below for an illustration. Warning The @depends_on decorator is only effective if the decorated step function is used \"as is\" as an argument in @test_steps() . If a non-direct relation is used, such as using the test step name as argument, you should use a shared data container (see below) to manually create the dependency. c- Shared data \u00b6 In this explicit mode, by default all steps are independent, therefore they do not have access to each other's execution results . To solve this problem, you can add a steps_data argument to your test function. If you do so, a StepsDataHolder object will be injected in this variable, that you can use to store and retrieve results. Simply create fields on it and store whatever you like: import pytest from pytest_steps import test_steps @test_steps ( 'step_a' , 'step_b' ) def test_suite_with_shared_results ( test_step , steps_data ): # Execute the step with access to the steps_data holder if test_step == 'step_a' : step_a ( steps_data ) elif test_step == 'step_b' : step_b ( steps_data ) def step_a ( steps_data ): # perform this step ... print ( \"step a\" ) assert not False # replace with your logic # intermediate results can be stored in steps_data steps_data . intermediate_a = 'hello' def step_b ( steps_data ): # perform this step, leveraging the previous step's results print ( \"step b\" ) # you can leverage the results from previous steps... # ... or pytest.skip if not relevant if len ( steps_data . intermediate_a ) < 5 : pytest . skip ( \"Step b should only be executed if the text is long enough\" ) new_text = steps_data . intermediate_a + \" ... augmented\" print ( new_text ) assert len ( new_text ) == 56 d- Calling decorated functions manually \u00b6 In \"explicit\" mode it is possible to call your test functions outside of pytest runners, exactly the same way we saw in generator mode . An exemple can be found here . e- Compliance with pytest \u00b6 You can add as many @pytest.mark.parametrize and pytest fixtures in your test suite function, it should work as expected: a new steps_data object will be created everytime a new parameter/fixture combination is created, and that object will be shared across steps with the same parameters and fixtures. Concerning fixtures, by default all function-scoped fixtures will be \"one per step\" in this mode (you do not even need to use the @one_fixture_per_step decorator - although it does not hurt). if you wish a fixture to be shared across several steps, decorate it with @cross_steps_fixture . For example from pytest_steps import cross_steps_fixture usage_counter = 0 @pytest . fixture @cross_steps_fixture def my_cool_fixture (): \"\"\"A fixture that returns a new integer every time it is used.\"\"\" global usage_counter usage_counter += 1 print ( \"created my_fixture %s \" % usage_counter ) return usage_counter def step_a (): print ( 'hello' ) def step_b (): print ( 'world' ) @test_steps ( step_a , step_b ) def test_params_mode ( test_step , my_cool_fixture ): # assert that whatever the step, the fixture is the same (shared across steps) assert my_cool_fixture == 1 test_step () 3. Usage with pytest-harvest \u00b6 a- Enhancing the results df \u00b6 You might already use pytest-harvest to turn your tests into functional benchmarks. When you combine it with pytest_steps you end up with one row in the synthesis table per step . For example: test_id status duration_ms _ step_name algo_param dataset accuracy test_my_app_bench[A-1-train] passed 2.00009 train 1 my dataset #A 0.832642 test_my_app_bench[A-1-score] passed 0 score 1 my dataset #A nan test_my_app_bench[A-2-train] passed 1.00017 train 2 my dataset #A 0.0638134 test_my_app_bench[A-2-score] passed 0.999928 score 2 my dataset #A nan test_my_app_bench[B-1-train] passed 0 train 1 my dataset #B 0.870705 test_my_app_bench[B-1-score] passed 0 score 1 my dataset #B nan test_my_app_bench[B-2-train] passed 0 train 2 my dataset #B 0.764746 test_my_app_bench[B-2-score] passed 1.0004 score 2 my dataset #B nan You might wish to use the provided handle_steps_in_results_df utility method to replace the index with a 2-level multiindex (test id without step, step id). b- Pivoting the results df \u00b6 If you prefer to see one row per test and the step details in columns, this package also provides NEW default [module/session]_results_df_steps_pivoted fixtures to directly get the pivoted version ; and a pivot_steps_on_df utility method to perform the pivot transform easily. You will for example obtain this kind of pivoted table: test_id algo_param dataset train/status train/duration_ms train/accuracy score/status score/duration_ms test_my_app_bench[A-1] 1 my dataset #A passed 2.00009 0.832642 passed 0 test_my_app_bench[A-2] 2 my dataset #A passed 1.00017 0.0638134 passed 0.999928 test_my_app_bench[B-1] 1 my dataset #B passed 0 0.870705 passed 0 test_my_app_bench[B-2] 2 my dataset #B passed 0 0.764746 passed 1.0004 c- Examples \u00b6 Two examples are available that should be quite straightforward for those familiar with pytest-harvest: here an example relying on default fixtures, to show how simple it is to satisfy the most common use cases. here an advanced example where the custom synthesis is created manually from the dictionary provided by pytest-harvest, thanks to helper methods. Main features / benefits \u00b6 Split tests into steps . Although the best practices in testing are very much in favor of having each test completely independent of the other ones (for example for distributed execution), there is definitely some value in results readability to break down tests into chained sub-tests (steps). The @test_steps decorator provides an intuitive way to do that without forcing any data model (steps can be functions, objects, etc.). Multi-style : an explicit mode and a generator mode are supported, developers may wish to use one or the other depending on their coding style or readability target. Steps can share data - In generator mode this is out-of-the-box. In explicit mode all steps in the same test suite can share data through the injected steps_data container (name is configurable). Steps dependencies can be defined : a @depends_on decorator ( explicit mode) or an optional_step context manager ( generator mode) allow you to specify that a given test step should be skipped or failed if its dependencies did not complete. See Also \u00b6 pytest documentation on parametrize pytest documentation on fixtures pytest-cases , to go further and separate test data from test functions Others \u00b6 Do you like this library ? You might also like my other python libraries Want to contribute ? \u00b6 Details on the github page: https://github.com/smarie/python-pytest-steps","title":"Home"},{"location":"#pytest-steps","text":"Create step-wise / incremental tests in pytest . Manual execution of tests has slightly changed in 1.7.0 , see explanations here New 'generator' style is there, check it out ! New pytest-harvest compatibility fixtures, check them out ! Did you ever want to organize your test in incremental steps, for example to improve readability in case of failure ? Or to have some optional steps, executing only conditionally to previous steps' results? pytest-steps leverages pytest and its great @pytest.mark.parametrize and @pytest.fixture decorators, so that you can create incremental tests with steps without having to think about the pytest fixture/parametrize pattern that has to be implemented for your particular case. This is particularly helpful if: you wish to share a state / intermediate results across test steps your tests already rely on other fixtures and/or parameters , such as with pytest-cases . In that case, finding the correct pytest design, that will ensure that you have a brand new state object at the beginning of each test suite while ensuring that this object will actually be shared across all steps, might be very tricky. With pytest-steps you don't have to care about the internals: it just works as expected. Note pytest-steps has not yet been tested with pytest-xdist. See #7","title":"pytest-steps"},{"location":"#installing","text":"> pip install pytest_steps","title":"Installing"},{"location":"#1-usage-generator-mode","text":"This new mode may seem more natural and readable to non-pytest experts. However it may be harder to debug when used in combination with other pytest tricks. In such case, do not hesitate to switch to good old \"explicit\" mode .","title":"1. Usage - \"generator\" mode"},{"location":"#a-basics","text":"Start with you favorite test function. There are two things to do, to break it down into steps: decorate it with @test_steps to declare what are the steps that will be performed, as strings. insert as many yield statements in your function body as there are steps. The function should end with a yield (not return !). !!! note Code written after the last yield will not be executed. For example we define three steps: from pytest_steps import test_steps @test_steps ( 'step_a' , 'step_b' , 'step_c' ) def test_suite (): # Step A print ( \"step a\" ) assert not False # replace with your logic intermediate_a = 'hello' yield # Step B print ( \"step b\" ) assert not False # replace with your logic yield # Step C print ( \"step c\" ) new_text = intermediate_a + \" ... augmented\" print ( new_text ) assert len ( new_text ) == 56 yield That's it! If you run pytest you will now see 3 tests instead of one: ============================= test session starts ============================= (...) collected 3 items (...)/test_example.py::test_suite[step_a] <- <decorator-gen-3> PASSED [ 33%] (...)/test_example.py::test_suite[step_b] <- <decorator-gen-3> PASSED [ 66%] (...)/test_example.py::test_suite[step_c] <- <decorator-gen-3> PASSED [100%] ========================== 3 passed in 0.06 seconds =========================== Debugging note You might wish to use yield <step_name> instead of yield at the end of each step when debugging if you think that there is an issue with the execution order. This will activate a built-in checker, that will check that each step name in the declared sequence corresponds to what you actually yield at the end of that step.","title":"a- Basics"},{"location":"#b-shared-data","text":"By design, all intermediate results created during function execution are shared between steps, since they are part of the same python function call. You therefore have nothing to do: this is what is shown above in step c where we reuse intermediate_a from step a.","title":"b- Shared data"},{"location":"#c-optional-steps-and-dependencies","text":"In this generator mode, all steps depend on all previous steps by default: if a step fails, all subsequent steps will be skipped. To circumvent this behaviour you can declare a step as optional. This means that subsequent steps will not depend on it except explicitly stated. For this you should: wrap the step into the special optional_step context manager, yield the corresponding context object at the end of the step, instead of None or the step name. This is very important, otherwise the step will be considered as successful by pytest! For example: # Step B with optional_step ( 'step_b' ) as step_b : print ( \"step b\" ) assert False yield step_b If steps depend on an optional step in order to execute, you should make them optional too, and state it explicitly: declare the dependency using the depends_on argument. use should_run() to test if the code block should be executed. The example below shows 4 steps, where steps a and d are mandatory and b and c are optional with c dependent on b: from pytest_steps import test_steps , optional_step @test_steps ( 'step_a' , 'step_b' , 'step_c' , 'step_d' ) def test_suite_opt (): # Step A assert not False yield # Step B with optional_step ( 'step_b' ) as step_b : assert False yield step_b # Step C depends on step B with optional_step ( 'step_c' , depends_on = step_b ) as step_c : if step_c . should_run (): assert True yield step_c # Step D assert not False yield Running it with pytest shows the desired behaviour: step b fails but does not prevent step d to execute correctly. step c is marked as skipped because its dependency (step b) failed. ============================= test session starts ============================= (...) collected 4 items (...)/test_example.py::test_suite_opt[step_a] <- <decorator-gen-3> PASSED [ 25%] (...)/test_example.py::test_suite_opt[step_b] <- <decorator-gen-3> FAILED [ 50%] (...)/test_example.py::test_suite_opt[step_c] <- <decorator-gen-3> SKIPPED [ 75%] (...)/test_example.py::test_suite_opt[step_d] <- <decorator-gen-3> PASSED [100%] ================================== FAILURES =================================== _______________ test_suite_optional_and_dependent_steps[step_b] _______________ (...) ================ 1 failed, 2 passed, 1 skipped in 0.16 seconds ================","title":"c- Optional steps and dependencies"},{"location":"#d-calling-decorated-functions-manually","text":"In some cases you might wish to call your test functions manually before the tests actually run. This can be very useful when you do not wish the package import times to be counted in test execution durations - typically in a \"benchmarking\" use case such as shown here . It is now possible to call a test function decorated with @test_steps manually. For this the best way to understand what you have to provide is to inspect it. from pytest_steps import test_steps @test_steps ( 'first' , 'second' ) def test_dummy (): print ( 'hello' ) yield print ( 'world' ) yield print ( help ( test_dummy )) yields test_dummy(________step_name_, request) So we have to provide two arguments: ________step_name_ and request . Note: the same information can be obtained in a more formal way using signature from the inspect (latest python) or funcsigs (older) packages. Once you know what arguments you have to provide, there are two rules to follow in order to execute the function manually: replace the step_name argument with which steps you wish to execute: None to execute all steps in order, or a list of steps to execute some steps only. Note that in generator mode, \"by design\" (generator function) it is only possible to call the steps in correct order and starting from the first one, but you can provide a partial list: replace the request argument with None , to indicate that you are executing outside of any pytest context. > test_dummy ( None , None ) hello world > test_dummy ( 'first' , None ) hello > test_dummy ( 'second' , None ) ValueError : Incorrect sequence of steps provided for manual execution . Step #1 should be named 'first', found 'second' arguments order changed in 1.7.0 Unfortunately the order of arguments for manual execution changed in version 1.7.0 . This was the only way to add support for class methods . Apologies !","title":"d- Calling decorated functions manually"},{"location":"#e-compliance-with-pytest","text":"","title":"e- Compliance with pytest"},{"location":"#parameters","text":"Under the hood, the @test_steps decorator simply generates a wrapper function around your function and mark it with @pytest.mark.parametrize . The function wrapper is created using the excellent decorator library, so all marks that exist on it are kept in the process, as well as its name and signature. Therefore @test_steps should be compliant with all native pytest mechanisms. For exemple you can use decorators such as @pytest.mark.parametrize before or after it in the function decoration order (depending on your desired resulting test order): @test_steps ( 'step_a' , 'step_b' ) @pytest . mark . parametrize ( 'i' , range ( 2 ), ids = lambda i : \"i= %i \" % i ) def test_suite_p ( i ): # Step A print ( \"step a, i= %i \" % i ) assert not False # replace with your logic yield # Step B print ( \"step b, i= %i \" % i ) assert not False # replace with your logic yield If you execute it, it correctly executes all the steps for each parameter value: ============================= test session starts ============================= (...) collected 4 items (...)/test_example.py::test_suite_p[i=0-step_a] <- <decorator-gen-3> PASSED [ 25%] (...)/test_example.py::test_suite_p[i=0-step_b] <- <decorator-gen-3> PASSED [ 50%] (...)/test_example.py::test_suite_p[i=1-step_a] <- <decorator-gen-3> PASSED [ 75%] (...)/test_example.py::test_suite_p[i=1-step_b] <- <decorator-gen-3> PASSED [100%] ========================== 4 passed in 0.07 seconds ===========================","title":"parameters"},{"location":"#fixtures","text":"You can also use fixtures as usual, but special care has to be taken about function-scope fixtures . Let's consider the following example: usage_counter = 0 @pytest . fixture def my_fixture (): \"\"\"Simple function-scoped fixture that return a new instance each time\"\"\" global usage_counter usage_counter += 1 print ( \"created my_fixture %s \" % usage_counter ) return usage_counter @test_steps ( 'step_a' , 'step_b' ) def test_suite_one_fixture_per_step ( my_fixture ): # Step A print ( \"step a\" ) assert my_fixture == 1 yield # Step B print ( \"step b\" ) assert my_fixture == 2 # >> raises an AssertionError because my_fixture = 1 ! yield Here, and that can be a bit misleading, pytest will call my_fixture() twice, because there are two pytest function executions, one for each step. So we think that everything is good... ...however the second fixture instance is never be passed to our test code: instead, the my_fixture instance that was passed as argument in the first step will be used by all steps. Therefore we end up having a failure in the test furing step b. It is possible to circumvent this behaviour by declaring explicitly what you expect: if you would like to share fixture instances across steps, decorate your fixture with @cross_steps_fixture . if you would like each step to have its own fixture instance, decorate your fixture with @one_fixture_per_step . For example from pytest_steps import one_fixture_per_step @pytest . fixture @one_fixture_per_step def my_fixture (): \"\"\"Simple function-scoped fixture that return a new instance each time\"\"\" global usage_counter usage_counter += 1 return usage_counter Each step will now use its own fixture instance and the test will succeed (instance 2 will be available at step b). When a fixture is decorated with @one_fixture_per_step , the object that is injected in your test function is a transparent proxy of the fixture, so it behaves exactly like the fixture. If for some reason you want to get the \"true\" inner wrapped object, you can do so using get_underlying_fixture(my_fixture) .","title":"fixtures"},{"location":"#2-usage-explicit-mode","text":"In \"explicit\" mode, things are a bit more complex to write but can be easier to understand because it does not use generators, just simple function calls.","title":"2. Usage - \"explicit\" mode"},{"location":"#a-basics_1","text":"Like for the other mode, simply decorate your test function with @test_steps and declare what are the steps that will be performed. In addition, put a test_step parameter in your function, that will receive the current step. The easiest way to use it is to declare each step as a function: from pytest_steps import test_steps def step_a (): # perform this step ... print ( \"step a\" ) assert not False # replace with your logic def step_b (): # perform this step print ( \"step b\" ) assert not False # replace with your logic @test_steps ( step_a , step_b ) def test_suite_1 ( test_step ): # Optional: perform step-specific reasoning, for example to select arguments if test_step . __name__ == \"step_a\" : print ( \"calling step a\" ) # Execute the step by calling the test step function test_step () Note: as shown above, you can perform some reasoning about the step at hand in test_suite_1 , by looking at the test_step object. Custom parameter name You might want another name than test_step to receive the current step. The test_step_argname argument can be used to change that name.","title":"a- Basics"},{"location":"#variants-other-types","text":"This mechanism is actually nothing more than a pytest parameter so it has to requirement on the test_step type. It is therefore possible to use other types, for example to declare the test steps as strings instead of function: @test_steps ( 'step_a' , 'step_b' ) def test_suite_2 ( test_step ): # Execute the step according to name if test_step == 'step_a' : step_a () elif test_step == 'step_b' : step_b () ... This has pros and cons: (+) you can declare the test suite before the step functions in the python file (better readability !) (-) you can not use @depends_on to decorate your step functions: you can only rely on shared data container to create dependencies (as explained below)","title":"Variants: other types"},{"location":"#b-auto-skipfail","text":"In this explicit mode all steps are optional/independent by default: each of them will be run, whatever the execution result of others. If you wish to change this, you can use the @depends_on decorator to mark a step as to be automatically skipped or failed if some other steps did not run successfully. For example: from pytest_steps import depends_on def step_a (): ... @depends_on ( step_a ) def step_b (): ... That way, step_b will now be skipped if step_a does not run successfully. Note that if you use shared data (see below), you can perform similar, and also more advanced dependency checks, by checking the contents of the shared data and calling pytest.skip() or pytest.fail() according to what is present. See step_b in the example below for an illustration. Warning The @depends_on decorator is only effective if the decorated step function is used \"as is\" as an argument in @test_steps() . If a non-direct relation is used, such as using the test step name as argument, you should use a shared data container (see below) to manually create the dependency.","title":"b- Auto-skip/fail"},{"location":"#c-shared-data","text":"In this explicit mode, by default all steps are independent, therefore they do not have access to each other's execution results . To solve this problem, you can add a steps_data argument to your test function. If you do so, a StepsDataHolder object will be injected in this variable, that you can use to store and retrieve results. Simply create fields on it and store whatever you like: import pytest from pytest_steps import test_steps @test_steps ( 'step_a' , 'step_b' ) def test_suite_with_shared_results ( test_step , steps_data ): # Execute the step with access to the steps_data holder if test_step == 'step_a' : step_a ( steps_data ) elif test_step == 'step_b' : step_b ( steps_data ) def step_a ( steps_data ): # perform this step ... print ( \"step a\" ) assert not False # replace with your logic # intermediate results can be stored in steps_data steps_data . intermediate_a = 'hello' def step_b ( steps_data ): # perform this step, leveraging the previous step's results print ( \"step b\" ) # you can leverage the results from previous steps... # ... or pytest.skip if not relevant if len ( steps_data . intermediate_a ) < 5 : pytest . skip ( \"Step b should only be executed if the text is long enough\" ) new_text = steps_data . intermediate_a + \" ... augmented\" print ( new_text ) assert len ( new_text ) == 56","title":"c- Shared data"},{"location":"#d-calling-decorated-functions-manually_1","text":"In \"explicit\" mode it is possible to call your test functions outside of pytest runners, exactly the same way we saw in generator mode . An exemple can be found here .","title":"d- Calling decorated functions manually"},{"location":"#e-compliance-with-pytest_1","text":"You can add as many @pytest.mark.parametrize and pytest fixtures in your test suite function, it should work as expected: a new steps_data object will be created everytime a new parameter/fixture combination is created, and that object will be shared across steps with the same parameters and fixtures. Concerning fixtures, by default all function-scoped fixtures will be \"one per step\" in this mode (you do not even need to use the @one_fixture_per_step decorator - although it does not hurt). if you wish a fixture to be shared across several steps, decorate it with @cross_steps_fixture . For example from pytest_steps import cross_steps_fixture usage_counter = 0 @pytest . fixture @cross_steps_fixture def my_cool_fixture (): \"\"\"A fixture that returns a new integer every time it is used.\"\"\" global usage_counter usage_counter += 1 print ( \"created my_fixture %s \" % usage_counter ) return usage_counter def step_a (): print ( 'hello' ) def step_b (): print ( 'world' ) @test_steps ( step_a , step_b ) def test_params_mode ( test_step , my_cool_fixture ): # assert that whatever the step, the fixture is the same (shared across steps) assert my_cool_fixture == 1 test_step ()","title":"e- Compliance with pytest"},{"location":"#3-usage-with-pytest-harvest","text":"","title":"3. Usage with pytest-harvest"},{"location":"#a-enhancing-the-results-df","text":"You might already use pytest-harvest to turn your tests into functional benchmarks. When you combine it with pytest_steps you end up with one row in the synthesis table per step . For example: test_id status duration_ms _ step_name algo_param dataset accuracy test_my_app_bench[A-1-train] passed 2.00009 train 1 my dataset #A 0.832642 test_my_app_bench[A-1-score] passed 0 score 1 my dataset #A nan test_my_app_bench[A-2-train] passed 1.00017 train 2 my dataset #A 0.0638134 test_my_app_bench[A-2-score] passed 0.999928 score 2 my dataset #A nan test_my_app_bench[B-1-train] passed 0 train 1 my dataset #B 0.870705 test_my_app_bench[B-1-score] passed 0 score 1 my dataset #B nan test_my_app_bench[B-2-train] passed 0 train 2 my dataset #B 0.764746 test_my_app_bench[B-2-score] passed 1.0004 score 2 my dataset #B nan You might wish to use the provided handle_steps_in_results_df utility method to replace the index with a 2-level multiindex (test id without step, step id).","title":"a- Enhancing the results df"},{"location":"#b-pivoting-the-results-df","text":"If you prefer to see one row per test and the step details in columns, this package also provides NEW default [module/session]_results_df_steps_pivoted fixtures to directly get the pivoted version ; and a pivot_steps_on_df utility method to perform the pivot transform easily. You will for example obtain this kind of pivoted table: test_id algo_param dataset train/status train/duration_ms train/accuracy score/status score/duration_ms test_my_app_bench[A-1] 1 my dataset #A passed 2.00009 0.832642 passed 0 test_my_app_bench[A-2] 2 my dataset #A passed 1.00017 0.0638134 passed 0.999928 test_my_app_bench[B-1] 1 my dataset #B passed 0 0.870705 passed 0 test_my_app_bench[B-2] 2 my dataset #B passed 0 0.764746 passed 1.0004","title":"b- Pivoting the results df"},{"location":"#c-examples","text":"Two examples are available that should be quite straightforward for those familiar with pytest-harvest: here an example relying on default fixtures, to show how simple it is to satisfy the most common use cases. here an advanced example where the custom synthesis is created manually from the dictionary provided by pytest-harvest, thanks to helper methods.","title":"c- Examples"},{"location":"#main-features-benefits","text":"Split tests into steps . Although the best practices in testing are very much in favor of having each test completely independent of the other ones (for example for distributed execution), there is definitely some value in results readability to break down tests into chained sub-tests (steps). The @test_steps decorator provides an intuitive way to do that without forcing any data model (steps can be functions, objects, etc.). Multi-style : an explicit mode and a generator mode are supported, developers may wish to use one or the other depending on their coding style or readability target. Steps can share data - In generator mode this is out-of-the-box. In explicit mode all steps in the same test suite can share data through the injected steps_data container (name is configurable). Steps dependencies can be defined : a @depends_on decorator ( explicit mode) or an optional_step context manager ( generator mode) allow you to specify that a given test step should be skipped or failed if its dependencies did not complete.","title":"Main features / benefits"},{"location":"#see-also","text":"pytest documentation on parametrize pytest documentation on fixtures pytest-cases , to go further and separate test data from test functions","title":"See Also"},{"location":"#others","text":"Do you like this library ? You might also like my other python libraries","title":"Others"},{"location":"#want-to-contribute","text":"Details on the github page: https://github.com/smarie/python-pytest-steps","title":"Want to contribute ?"},{"location":"api_reference/","text":"API reference \u00b6 In general, using help(symbol) is the recommended way to get the latest documentation. In addition, this page provides an overview of the various elements in this package. Both modes \u00b6 @test_steps \u00b6 @test_steps ( * steps , mode : str = 'auto' , test_step_argname : str = 'step_name' , steps_data_holder_name : str = 'steps_data' ) Decorates a test function so as to automatically parametrize it with all steps listed as arguments. There are two main ways to use this decorator: decorate a test function generator and provide as many step names as there are 'yield' statements in the generator decorate a test function with a 'test_step' parameter, and use this parameter in the test function body to decide what to execute. See Home for examples. Parameters: steps : a list of test steps. They can be anything, but typically they are non-test (not prefixed with 'test') functions. mode : one of {'auto', 'generator', 'parametrizer'} . In 'auto' mode (default), the decorator will detect if your function is a generator or not. If it is a generator it will use the generator mode, otherwise it will use the parametrizer (explicit) mode. test_step_argname : the optional name of the function argument that will receive the test step object. Default is 'test_step'. test_results_argname : the optional name of the function argument that will receive the shared StepsDataHolder object if present. Default is 'steps_data'. @cross_steps_fixture \u00b6 A decorator for a function-scoped fixture so that it is not called for each step, but only once for all steps. Decorating your fixture with @cross_steps_fixture tells @test_steps to detect when the fixture function is called for the first step, to cache that first step instance, and to reuse it instead of calling your fixture function for subsequent steps. This results in all steps (with the same other parameters) using the same fixture instance. Everything that is placed below this decorator will be called only once for all steps. For example if you use it in combination with @saved_fixture from pytest-harvest you will get the two possible behaviours below depending on the order of the decorators: Case A (recommended): @saved_fixture will be executed for all steps, and the saved object will be the same for all steps (since it will be cached by @cross_steps_fixture ) @pytest . fixture @saved_fixture @cross_steps_fixture def my_cool_fixture (): return random () Case B: @saved_fixture will only be called for the first step. Indeed for subsequent steps, @cross_steps_fixture will directly return and prevent the underlying functions to be called. This is not a very interesting behaviour in this case, but with other decorators it might be interesting. @pytest . fixture @cross_steps_fixture @saved_fixture def my_cool_fixture (): return random () If you use custom test step parameter names and not the default, you will have to provide an exhaustive list in the step_param_names argument. @one_fixture_per_step \u00b6 A decorator for a function-scoped fixture so that it works well with generator-mode test functions. You do not have to use it in parametrizer mode, although it does not hurt. By default if you do not use this decorator but use the fixture in a generator-mode test function, only the fixture created for the first step will be injected in your test function, and all subsequent steps will see that same instance. Decorating your fixture with @one_fixture_per_step tells @test_steps to transparently replace the fixture object instance by the one created for each step, before each step executes in your test function. This results in all steps using different fixture instances, as expected. It is recommended that you put this decorator as the second decorator, right after @pytest.fixture : @pytest . fixture @one_fixture_per_step def my_cool_fixture (): return random () When a fixture is decorated with @one_fixture_per_step , the object that is injected in your test function is a transparent proxy of the fixture, so it behaves exactly like the fixture. If for some reason you want to get the \"true\" inner wrapped object, you can do so using get_underlying_fixture(my_fixture) . Generator mode \u00b6 with optional_step \u00b6 with optional_step ( step_name : str , depends_on : Union [ optional_step , Iterable [ optional_step ]] = None ) Context manager to use inside a test function body to create an optional step named step_name with optional dependencies on other optional steps. See Home for examples. Parameters: step_name : the name of this optional step. This name will be used in pytest failure/skip messages when other steps depend on this one and are skipped/failed because this one was skipped/failed. depends_on : an optional dependency or list of dependencies, that should all be optional steps created with an optional_step context manager. Explicit/parametrizer mode \u00b6 @depends_on \u00b6 @depends_on ( * steps , fail_instead_of_skip : bool = False ) Decorates a test step object/function so as to automatically mark it as skipped (default) or failed if the dependency has not succeeded. This only works if the decorated object is directly used as argument in the main @test_steps decorator. Otherwise you can still use the shared results holder to skip manually, see Home for examples. Parameters: steps : a list of test steps that this step depends on. They can be anything, but typically they are non-test (not prefixed with 'test') functions. fail_instead_of_skip : if set to True, the test will be marked as failed instead of skipped when the dependencies have not succeeded. pytest-harvest utility methods \u00b6 handle_steps_in_results_dct \u00b6 def handle_steps_in_results_dct ( results_dct , is_flat = False , raise_if_one_test_without_step_id = False , no_step_id = '-' , step_param_names = None , keep_orig_id = True , no_steps_policy = 'raise' ) Improves the synthesis dictionary so that the keys are replaced with a tuple (new_test_id, step_id) where new_test_id is a step-independent test id the 'step_id' parameter is removed from the contents is_flat should be set to True if the dictionary has been flattened by pytest-harvest . The step id is identified by looking at the pytest parameters, and finding one with a name included in the step_param_names list ( None uses the default names). If no step id is found on an entry, it is replaced with the value of no_step_id except if raise_if_one_test_without_step_id=True - in which case an error is raised. If all step ids are missing, for all entries in the dictionary, no_steps_policy determines what happens: it can either skip the whole function and return a copy of the input ('skip', or behave as usual ('ignore'), or raise an error ('raise'). If keep_orig_id is set to True (default), the original id is added to each entry. handle_steps_in_results_df \u00b6 def handle_steps_in_results_df ( results_df , raise_if_one_test_without_step_id = False , # type: bool no_step_id = '-' , # type: str step_param_names = None , # type: Union[str, Iterable[str]] keep_orig_id = True , # type: bool no_steps_policy = 'raise' , # type: str inplace = False ): Improves the synthesis dataframe so that the test_id index is replaced with a multilevel index (new_test_id, step_id) where new_test_id is a step-independent test id. A 'pytest_id' column remains with the original id except if keep_orig_id=False (default=True) the 'step_id' parameter is removed from the contents The step id is identified by looking at the columns, and finding one with a name included in the step_param_names list ( None uses the default names). If no step id is found on an entry, it is replaced with the value of no_step_id except if raise_if_one_test_without_step_id=True - in which case an error is raised. If all step ids are missing, for all entries in the dictionary, no_steps_policy determines what happens: it can either skip the whole function and return a copy of the input ('skip', or behave as usual ('ignore'), or raise an error ('raise'). If keep_orig_id is set to True (default), the original id is added as a new column. If inplace is False (default), a new dataframe will be returned. Otherwise the input dataframe will be modified inplace and nothing will be returned. pivot_steps_on_df \u00b6 def pivot_steps_on_df ( results_df , pytest_session = None , cross_steps_columns = None , # type: List[str] error_if_not_present = True # type: bool ): Pivots the dataframe so that there is one row per pytest_obj[params except step id] containing all steps info. The input dataframe should have a multilevel index with two levels (test id, step id) and with names ( results_df.index.names should be set). The test id should be independent on the step id. flatten_multilevel_columns \u00b6 def flatten_multilevel_columns ( df , sep = '/' # type: str ): Replaces the multilevel columns (typically after a pivot) with single-level ones, where the names contain all levels concatenated with the separator sep . For example when the two levels are foo and bar , the single level becomes foo/bar . This method is a shortcut for df.columns = get_flattened_multilevel_columns(df) . Lower-level methods \u00b6 remove_step_from_test_id \u00b6 get_all_pytest_param_names_except_step_id \u00b6 get_flattened_multilevel_columns \u00b6","title":"API reference"},{"location":"api_reference/#api-reference","text":"In general, using help(symbol) is the recommended way to get the latest documentation. In addition, this page provides an overview of the various elements in this package.","title":"API reference"},{"location":"api_reference/#both-modes","text":"","title":"Both modes"},{"location":"api_reference/#test_steps","text":"@test_steps ( * steps , mode : str = 'auto' , test_step_argname : str = 'step_name' , steps_data_holder_name : str = 'steps_data' ) Decorates a test function so as to automatically parametrize it with all steps listed as arguments. There are two main ways to use this decorator: decorate a test function generator and provide as many step names as there are 'yield' statements in the generator decorate a test function with a 'test_step' parameter, and use this parameter in the test function body to decide what to execute. See Home for examples. Parameters: steps : a list of test steps. They can be anything, but typically they are non-test (not prefixed with 'test') functions. mode : one of {'auto', 'generator', 'parametrizer'} . In 'auto' mode (default), the decorator will detect if your function is a generator or not. If it is a generator it will use the generator mode, otherwise it will use the parametrizer (explicit) mode. test_step_argname : the optional name of the function argument that will receive the test step object. Default is 'test_step'. test_results_argname : the optional name of the function argument that will receive the shared StepsDataHolder object if present. Default is 'steps_data'.","title":"@test_steps"},{"location":"api_reference/#cross_steps_fixture","text":"A decorator for a function-scoped fixture so that it is not called for each step, but only once for all steps. Decorating your fixture with @cross_steps_fixture tells @test_steps to detect when the fixture function is called for the first step, to cache that first step instance, and to reuse it instead of calling your fixture function for subsequent steps. This results in all steps (with the same other parameters) using the same fixture instance. Everything that is placed below this decorator will be called only once for all steps. For example if you use it in combination with @saved_fixture from pytest-harvest you will get the two possible behaviours below depending on the order of the decorators: Case A (recommended): @saved_fixture will be executed for all steps, and the saved object will be the same for all steps (since it will be cached by @cross_steps_fixture ) @pytest . fixture @saved_fixture @cross_steps_fixture def my_cool_fixture (): return random () Case B: @saved_fixture will only be called for the first step. Indeed for subsequent steps, @cross_steps_fixture will directly return and prevent the underlying functions to be called. This is not a very interesting behaviour in this case, but with other decorators it might be interesting. @pytest . fixture @cross_steps_fixture @saved_fixture def my_cool_fixture (): return random () If you use custom test step parameter names and not the default, you will have to provide an exhaustive list in the step_param_names argument.","title":"@cross_steps_fixture"},{"location":"api_reference/#one_fixture_per_step","text":"A decorator for a function-scoped fixture so that it works well with generator-mode test functions. You do not have to use it in parametrizer mode, although it does not hurt. By default if you do not use this decorator but use the fixture in a generator-mode test function, only the fixture created for the first step will be injected in your test function, and all subsequent steps will see that same instance. Decorating your fixture with @one_fixture_per_step tells @test_steps to transparently replace the fixture object instance by the one created for each step, before each step executes in your test function. This results in all steps using different fixture instances, as expected. It is recommended that you put this decorator as the second decorator, right after @pytest.fixture : @pytest . fixture @one_fixture_per_step def my_cool_fixture (): return random () When a fixture is decorated with @one_fixture_per_step , the object that is injected in your test function is a transparent proxy of the fixture, so it behaves exactly like the fixture. If for some reason you want to get the \"true\" inner wrapped object, you can do so using get_underlying_fixture(my_fixture) .","title":"@one_fixture_per_step"},{"location":"api_reference/#generator-mode","text":"","title":"Generator mode"},{"location":"api_reference/#with-optional_step","text":"with optional_step ( step_name : str , depends_on : Union [ optional_step , Iterable [ optional_step ]] = None ) Context manager to use inside a test function body to create an optional step named step_name with optional dependencies on other optional steps. See Home for examples. Parameters: step_name : the name of this optional step. This name will be used in pytest failure/skip messages when other steps depend on this one and are skipped/failed because this one was skipped/failed. depends_on : an optional dependency or list of dependencies, that should all be optional steps created with an optional_step context manager.","title":"with optional_step"},{"location":"api_reference/#explicitparametrizer-mode","text":"","title":"Explicit/parametrizer mode"},{"location":"api_reference/#depends_on","text":"@depends_on ( * steps , fail_instead_of_skip : bool = False ) Decorates a test step object/function so as to automatically mark it as skipped (default) or failed if the dependency has not succeeded. This only works if the decorated object is directly used as argument in the main @test_steps decorator. Otherwise you can still use the shared results holder to skip manually, see Home for examples. Parameters: steps : a list of test steps that this step depends on. They can be anything, but typically they are non-test (not prefixed with 'test') functions. fail_instead_of_skip : if set to True, the test will be marked as failed instead of skipped when the dependencies have not succeeded.","title":"@depends_on"},{"location":"api_reference/#pytest-harvest-utility-methods","text":"","title":"pytest-harvest utility methods"},{"location":"api_reference/#handle_steps_in_results_dct","text":"def handle_steps_in_results_dct ( results_dct , is_flat = False , raise_if_one_test_without_step_id = False , no_step_id = '-' , step_param_names = None , keep_orig_id = True , no_steps_policy = 'raise' ) Improves the synthesis dictionary so that the keys are replaced with a tuple (new_test_id, step_id) where new_test_id is a step-independent test id the 'step_id' parameter is removed from the contents is_flat should be set to True if the dictionary has been flattened by pytest-harvest . The step id is identified by looking at the pytest parameters, and finding one with a name included in the step_param_names list ( None uses the default names). If no step id is found on an entry, it is replaced with the value of no_step_id except if raise_if_one_test_without_step_id=True - in which case an error is raised. If all step ids are missing, for all entries in the dictionary, no_steps_policy determines what happens: it can either skip the whole function and return a copy of the input ('skip', or behave as usual ('ignore'), or raise an error ('raise'). If keep_orig_id is set to True (default), the original id is added to each entry.","title":"handle_steps_in_results_dct"},{"location":"api_reference/#handle_steps_in_results_df","text":"def handle_steps_in_results_df ( results_df , raise_if_one_test_without_step_id = False , # type: bool no_step_id = '-' , # type: str step_param_names = None , # type: Union[str, Iterable[str]] keep_orig_id = True , # type: bool no_steps_policy = 'raise' , # type: str inplace = False ): Improves the synthesis dataframe so that the test_id index is replaced with a multilevel index (new_test_id, step_id) where new_test_id is a step-independent test id. A 'pytest_id' column remains with the original id except if keep_orig_id=False (default=True) the 'step_id' parameter is removed from the contents The step id is identified by looking at the columns, and finding one with a name included in the step_param_names list ( None uses the default names). If no step id is found on an entry, it is replaced with the value of no_step_id except if raise_if_one_test_without_step_id=True - in which case an error is raised. If all step ids are missing, for all entries in the dictionary, no_steps_policy determines what happens: it can either skip the whole function and return a copy of the input ('skip', or behave as usual ('ignore'), or raise an error ('raise'). If keep_orig_id is set to True (default), the original id is added as a new column. If inplace is False (default), a new dataframe will be returned. Otherwise the input dataframe will be modified inplace and nothing will be returned.","title":"handle_steps_in_results_df"},{"location":"api_reference/#pivot_steps_on_df","text":"def pivot_steps_on_df ( results_df , pytest_session = None , cross_steps_columns = None , # type: List[str] error_if_not_present = True # type: bool ): Pivots the dataframe so that there is one row per pytest_obj[params except step id] containing all steps info. The input dataframe should have a multilevel index with two levels (test id, step id) and with names ( results_df.index.names should be set). The test id should be independent on the step id.","title":"pivot_steps_on_df"},{"location":"api_reference/#flatten_multilevel_columns","text":"def flatten_multilevel_columns ( df , sep = '/' # type: str ): Replaces the multilevel columns (typically after a pivot) with single-level ones, where the names contain all levels concatenated with the separator sep . For example when the two levels are foo and bar , the single level becomes foo/bar . This method is a shortcut for df.columns = get_flattened_multilevel_columns(df) .","title":"flatten_multilevel_columns"},{"location":"api_reference/#lower-level-methods","text":"","title":"Lower-level methods"},{"location":"api_reference/#remove_step_from_test_id","text":"","title":"remove_step_from_test_id"},{"location":"api_reference/#get_all_pytest_param_names_except_step_id","text":"","title":"get_all_pytest_param_names_except_step_id"},{"location":"api_reference/#get_flattened_multilevel_columns","text":"","title":"get_flattened_multilevel_columns"},{"location":"changelog/","text":"Changelog \u00b6 1.7.3 - packaging improvements \u00b6 packaging improvements: set the \"universal wheel\" flag to 1, and cleaned up the setup.py . In particular removed dependency to six for setup and added py.typed file, as well as set the zip_safe flag to False. Removed tests folder from package. Fixes #39 1.7.2 - warning removed \u00b6 Removed import warning. Fixed #37 1.7.1 - pyproject.toml \u00b6 Added pyproject.toml . 1.7.0 - Support for test functions located inside test classes \u00b6 @test_steps can now be used on test functions located inside classes. Fixed #16 Warning : as a consequence of the fix above, the order of arguments has changed. this has an impact for manual execution. See here for details. 1.6.4 - python 2 bugfix \u00b6 Fixed issue happening with python 2 when unicode_literals are used in the parameters receiving string types. Fixed #34 1.6.3 - added __version__ attribute \u00b6 Added __version__ attribute at package level 1.6.2 - added six dependency \u00b6 It was missing from setup.py . 1.6.1 - Minor code improvements \u00b6 Made the python 3 signature patch more readable... for those users who will enter in the code while debugging. 1.6.0 - Minor dependencies update \u00b6 Improved docstring for @cross_steps_fixture . Replaced decorator dependency + internal hack with proper usage of makefun . 1.5.4 - Bug fix \u00b6 The test step list is now correctly taken into account when a decorated function is called manually. Fixed #30 . 1.5.3 - Bug fix \u00b6 Fixed plugin initialization error when pytest_harvest is not present. Fixed #29 . 1.5.2 - Bug fix \u00b6 pytest_harvest is not anymore required for install. Fixed #28 . 1.5.1 - Bug fix and exceptions improvement \u00b6 We now detect that @cross_step_fixture or @one_fixture_per_step is applied on a fixture with the wrong scope, and raise a much more readable exception. Fixes #25 . Improved pivot_steps_on_df so that we can use a filter on it, and so that only cross-step fixtures are used in the default cross-step columns. Fixes #26 1.5.0 - New @cross_steps_fixture decorator \u00b6 @one_per_step renamed @one_fixture_per_step for clarity. Old alias will remain across at least one minor version. New @cross_steps_fixture decorator to declare that a function-scoped fixture should be created once and reused across all steps. This decorator and the already existing decorator @one_fixture_per_step provide a consistent and very intuitive way for users to declare how fixtures should behave in presence of steps. Fixes #24 . Minor: _get_step_param_names_or_default moved to steps submodule. 1.4.0 - Documentation + Possibility to call a decorated test function manually. \u00b6 New features: - It is now possible to call a test function decorated with @test_steps manually, for example to run it once at the beinning of a test session in order for all imports to be done before actual execution. Fixes #22 Minor: - steps_harvest_df_utils submodule is now correctly listed in __all__ . - Improved docstrings and documentation page for API reference. 1.3.0 - Default fixtures for pytest-harvest \u00b6 When steps are present, we now offer session_results_df_steps_pivoted and module_results_df_steps_pivoted default fixtures, to align with pytest-harvest >= 1.1 default fixtures session_results_df and module_results_df . Fixes #23 . Improved API to manipulate pytest-harvest results objects in presence of steps: - Renamed handle_steps_in_synthesis_dct into handle_steps_in_results_dct (old alias is kept for this version). Renamed parameter raise_if_no_step to raise_if_one_test_without_step_id . Added a parameter keep_orig_id , by default (True) the original test id is kept for reference. Another parameter no_steps_policy allows users to create_function the method transparent if no steps are found. - new method handle_steps_in_results_df to perform the same things than handle_steps_in_results_dct but directly on the synthesis dataframe. The parameters are almost the same. - New method flatten_multilevel_columns to diretly apply get_flattened_multilevel_columns on the columns of a dataframe - pivot_steps_on_df now has the ability to detect parameter and fixture names from the provided pytest session, so as not to pivot them (they should be stable across steps). It also provides an error_if_not_present parameter 1.2.1 - Alignment with pytest-harvest 1.2.1 \u00b6 pytest-harvest 1.2 provides default fixtures and fixes a few issues in the synthesis dictionary (in particular fixture and fixture parameters were overlapping each other). We aligned pytest-steps to leverage it. Also, minor improvement: the unique id internally generated for each test now includes the pytest object. In practice this does not change anything for most use cases, but it might allow later refactoring, and better diagnostics. 1.2.0 - Internal refactoring: we now use a more robust method to identify tests that are steps of the same test. \u00b6 This fixes some bugs that were happening on edge cases where several parameters had the same string id representation (or one was a substring of the other). Fixed #21 . 1.1.2 - pytest-harvest is now an optional dependency \u00b6 Fixed #20 1.1.1 - fixed ordering issue in generator mode \u00b6 Fixed a pytest ordering issue in generator mode, by relying on place_as . Fixed #18 . 1.1.0 - pytest-harvest utilities + @one_per_step fix \u00b6 Fixed: @one_per_step can now be used with generator-style fixtures. API: - New method get_underlying_fixture to Truly get a fixture value even if it comes from a @one_per_step - internal constant INNER_STEP_ARGNAME is now named GENERATOR_MODE_STEP_ARGNAME - 5 new utility methods to support combining this plugin with pytest-harvest (see documentation for details): handle_steps_in_results_dct , remove_step_from_test_id , get_all_pytest_param_names_except_step_id , pivot_steps_on_df , get_flattened_multilevel_columns 1.0.4 in progress - improved readability \u00b6 Improved readability in signature-fiddling hacks: now the logic is separate from the two generated function signatures, both for generator and parametrizer modes. 1.0.3 - fix: request in arguments with new generator mode \u00b6 Test functions using new generator mode can now use the 'request' parameter. Fixed #12 1.0.2 - fix for old version of decorator lib \u00b6 Fixed #11 1.0.1 - removed deprecation warnings \u00b6 Removed some deprecation warnings appearing in latest pytest 3.x, about the future pytest 4 to come. Fixed #10 1.0.0 - new \"generator\" mode + pytest 2.x compliance \u00b6 You can now implement your test steps as yield statements in a generator. See documentation for details. Closes #6 Parametrized mode now works with older version of pytest (where @pytest.fixture did not have a name= parameter). Fixes #9 0.7.2 - minor encoding issue in setup.py \u00b6 0.7.1 - Fixed regression on python 3 \u00b6 Python 3: After last tag a new bug appeared: an empty test named test_steps was created. Fixed it #5 . 0.7.0 - Python 2 support \u00b6 0.6.0 - New @depends_on decorator \u00b6 Added a first version of @depends_on decorator. Fixes #1 0.5.0 - First public version \u00b6 Initial fork from pytest-cases A few renames for readability: ResultsHolder becomes StepsDataHolder , and the default name for the holder becomes 'steps_data' . Documentation","title":"Changelog"},{"location":"changelog/#changelog","text":"","title":"Changelog"},{"location":"changelog/#173-packaging-improvements","text":"packaging improvements: set the \"universal wheel\" flag to 1, and cleaned up the setup.py . In particular removed dependency to six for setup and added py.typed file, as well as set the zip_safe flag to False. Removed tests folder from package. Fixes #39","title":"1.7.3 - packaging improvements"},{"location":"changelog/#172-warning-removed","text":"Removed import warning. Fixed #37","title":"1.7.2 - warning removed"},{"location":"changelog/#171-pyprojecttoml","text":"Added pyproject.toml .","title":"1.7.1 - pyproject.toml"},{"location":"changelog/#170-support-for-test-functions-located-inside-test-classes","text":"@test_steps can now be used on test functions located inside classes. Fixed #16 Warning : as a consequence of the fix above, the order of arguments has changed. this has an impact for manual execution. See here for details.","title":"1.7.0 - Support for test functions located inside test classes"},{"location":"changelog/#164-python-2-bugfix","text":"Fixed issue happening with python 2 when unicode_literals are used in the parameters receiving string types. Fixed #34","title":"1.6.4 - python 2 bugfix"},{"location":"changelog/#163-added-__version__-attribute","text":"Added __version__ attribute at package level","title":"1.6.3 - added __version__ attribute"},{"location":"changelog/#162-added-six-dependency","text":"It was missing from setup.py .","title":"1.6.2 - added six dependency"},{"location":"changelog/#161-minor-code-improvements","text":"Made the python 3 signature patch more readable... for those users who will enter in the code while debugging.","title":"1.6.1 - Minor code improvements"},{"location":"changelog/#160-minor-dependencies-update","text":"Improved docstring for @cross_steps_fixture . Replaced decorator dependency + internal hack with proper usage of makefun .","title":"1.6.0 - Minor dependencies update"},{"location":"changelog/#154-bug-fix","text":"The test step list is now correctly taken into account when a decorated function is called manually. Fixed #30 .","title":"1.5.4 - Bug fix"},{"location":"changelog/#153-bug-fix","text":"Fixed plugin initialization error when pytest_harvest is not present. Fixed #29 .","title":"1.5.3 - Bug fix"},{"location":"changelog/#152-bug-fix","text":"pytest_harvest is not anymore required for install. Fixed #28 .","title":"1.5.2 - Bug fix"},{"location":"changelog/#151-bug-fix-and-exceptions-improvement","text":"We now detect that @cross_step_fixture or @one_fixture_per_step is applied on a fixture with the wrong scope, and raise a much more readable exception. Fixes #25 . Improved pivot_steps_on_df so that we can use a filter on it, and so that only cross-step fixtures are used in the default cross-step columns. Fixes #26","title":"1.5.1 - Bug fix and exceptions improvement"},{"location":"changelog/#150-new-cross_steps_fixture-decorator","text":"@one_per_step renamed @one_fixture_per_step for clarity. Old alias will remain across at least one minor version. New @cross_steps_fixture decorator to declare that a function-scoped fixture should be created once and reused across all steps. This decorator and the already existing decorator @one_fixture_per_step provide a consistent and very intuitive way for users to declare how fixtures should behave in presence of steps. Fixes #24 . Minor: _get_step_param_names_or_default moved to steps submodule.","title":"1.5.0 - New @cross_steps_fixture decorator"},{"location":"changelog/#140-documentation-possibility-to-call-a-decorated-test-function-manually","text":"New features: - It is now possible to call a test function decorated with @test_steps manually, for example to run it once at the beinning of a test session in order for all imports to be done before actual execution. Fixes #22 Minor: - steps_harvest_df_utils submodule is now correctly listed in __all__ . - Improved docstrings and documentation page for API reference.","title":"1.4.0 - Documentation + Possibility to call a decorated test function manually."},{"location":"changelog/#130-default-fixtures-for-pytest-harvest","text":"When steps are present, we now offer session_results_df_steps_pivoted and module_results_df_steps_pivoted default fixtures, to align with pytest-harvest >= 1.1 default fixtures session_results_df and module_results_df . Fixes #23 . Improved API to manipulate pytest-harvest results objects in presence of steps: - Renamed handle_steps_in_synthesis_dct into handle_steps_in_results_dct (old alias is kept for this version). Renamed parameter raise_if_no_step to raise_if_one_test_without_step_id . Added a parameter keep_orig_id , by default (True) the original test id is kept for reference. Another parameter no_steps_policy allows users to create_function the method transparent if no steps are found. - new method handle_steps_in_results_df to perform the same things than handle_steps_in_results_dct but directly on the synthesis dataframe. The parameters are almost the same. - New method flatten_multilevel_columns to diretly apply get_flattened_multilevel_columns on the columns of a dataframe - pivot_steps_on_df now has the ability to detect parameter and fixture names from the provided pytest session, so as not to pivot them (they should be stable across steps). It also provides an error_if_not_present parameter","title":"1.3.0 - Default fixtures for pytest-harvest"},{"location":"changelog/#121-alignment-with-pytest-harvest-121","text":"pytest-harvest 1.2 provides default fixtures and fixes a few issues in the synthesis dictionary (in particular fixture and fixture parameters were overlapping each other). We aligned pytest-steps to leverage it. Also, minor improvement: the unique id internally generated for each test now includes the pytest object. In practice this does not change anything for most use cases, but it might allow later refactoring, and better diagnostics.","title":"1.2.1 - Alignment with pytest-harvest 1.2.1"},{"location":"changelog/#120-internal-refactoring-we-now-use-a-more-robust-method-to-identify-tests-that-are-steps-of-the-same-test","text":"This fixes some bugs that were happening on edge cases where several parameters had the same string id representation (or one was a substring of the other). Fixed #21 .","title":"1.2.0 - Internal refactoring: we now use a more robust method to identify tests that are steps of the same test."},{"location":"changelog/#112-pytest-harvest-is-now-an-optional-dependency","text":"Fixed #20","title":"1.1.2 - pytest-harvest is now an optional dependency"},{"location":"changelog/#111-fixed-ordering-issue-in-generator-mode","text":"Fixed a pytest ordering issue in generator mode, by relying on place_as . Fixed #18 .","title":"1.1.1 - fixed ordering issue in generator mode"},{"location":"changelog/#110-pytest-harvest-utilities-one_per_step-fix","text":"Fixed: @one_per_step can now be used with generator-style fixtures. API: - New method get_underlying_fixture to Truly get a fixture value even if it comes from a @one_per_step - internal constant INNER_STEP_ARGNAME is now named GENERATOR_MODE_STEP_ARGNAME - 5 new utility methods to support combining this plugin with pytest-harvest (see documentation for details): handle_steps_in_results_dct , remove_step_from_test_id , get_all_pytest_param_names_except_step_id , pivot_steps_on_df , get_flattened_multilevel_columns","title":"1.1.0 - pytest-harvest utilities + @one_per_step fix"},{"location":"changelog/#104-in-progress-improved-readability","text":"Improved readability in signature-fiddling hacks: now the logic is separate from the two generated function signatures, both for generator and parametrizer modes.","title":"1.0.4 in progress - improved readability"},{"location":"changelog/#103-fix-request-in-arguments-with-new-generator-mode","text":"Test functions using new generator mode can now use the 'request' parameter. Fixed #12","title":"1.0.3 - fix: request in arguments with new generator mode"},{"location":"changelog/#102-fix-for-old-version-of-decorator-lib","text":"Fixed #11","title":"1.0.2 - fix for old version of decorator lib"},{"location":"changelog/#101-removed-deprecation-warnings","text":"Removed some deprecation warnings appearing in latest pytest 3.x, about the future pytest 4 to come. Fixed #10","title":"1.0.1 - removed deprecation warnings"},{"location":"changelog/#100-new-generator-mode-pytest-2x-compliance","text":"You can now implement your test steps as yield statements in a generator. See documentation for details. Closes #6 Parametrized mode now works with older version of pytest (where @pytest.fixture did not have a name= parameter). Fixes #9","title":"1.0.0 - new \"generator\" mode + pytest 2.x compliance"},{"location":"changelog/#072-minor-encoding-issue-in-setuppy","text":"","title":"0.7.2 - minor encoding issue in setup.py"},{"location":"changelog/#071-fixed-regression-on-python-3","text":"Python 3: After last tag a new bug appeared: an empty test named test_steps was created. Fixed it #5 .","title":"0.7.1 - Fixed regression on python 3"},{"location":"changelog/#070-python-2-support","text":"","title":"0.7.0 - Python 2 support"},{"location":"changelog/#060-new-depends_on-decorator","text":"Added a first version of @depends_on decorator. Fixes #1","title":"0.6.0 - New @depends_on decorator"},{"location":"changelog/#050-first-public-version","text":"Initial fork from pytest-cases A few renames for readability: ResultsHolder becomes StepsDataHolder , and the default name for the holder becomes 'steps_data' . Documentation","title":"0.5.0 - First public version"},{"location":"long_description/","text":"pytest-steps \u00b6 Create step-wise / incremental tests in pytest . The documentation for users is available here: https://smarie.github.io/python-pytest-steps/ A readme for developers is available here: https://github.com/smarie/python-pytest-steps","title":"pytest-steps"},{"location":"long_description/#pytest-steps","text":"Create step-wise / incremental tests in pytest . The documentation for users is available here: https://smarie.github.io/python-pytest-steps/ A readme for developers is available here: https://github.com/smarie/python-pytest-steps","title":"pytest-steps"}]}