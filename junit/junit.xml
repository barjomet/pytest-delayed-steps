<?xml version="1.0" encoding="utf-8"?><testsuite errors="0" failures="2" name="pytest" skips="0" tests="12" time="2.102"><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_steps_with_results_complex.py]" time="0.11119937896728516"><system-out>
Testing that running pytest on file test_steps_with_results_complex.py results in {&apos;passed&apos;: 16, &apos;failed&apos;: 0, &apos;skipped&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests0, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.0, cov-2.6.0
collected 16 items

test_run_all_tests.py ................                                   [100%]

========================== 16 passed in 0.06 seconds ===========================
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_wrapped_in_class.py]" time="0.06348037719726562"><system-out>
Testing that running pytest on file test_wrapped_in_class.py results in {&apos;passed&apos;: 1, &apos;failed&apos;: 0, &apos;skipped&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests1, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.0, cov-2.6.0
collected 1 item

test_run_all_tests.py .                                                  [100%]

=========================== 1 passed in 0.02 seconds ===========================
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_stackoverflow.py]" time="0.06882047653198242"><system-out>
Testing that running pytest on file test_stackoverflow.py results in {&apos;passed&apos;: 3, &apos;failed&apos;: 0, &apos;skipped&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests2, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.0, cov-2.6.0
collected 3 items

test_run_all_tests.py ...                                                [100%]

=========================== 3 passed in 0.02 seconds ===========================
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_steps_harvest.py]" time="0.546069860458374"><failure message="AssertionError: assert {&apos;error&apos;: 0, &apos;failed&apos;: 1, &apos;passed&apos;: 13, &apos;skipped&apos;: 0, ...} == {&apos;error&apos;: 0, &apos;failed&apos;: 0, &apos;passed&apos;: 14, &apos;skipped&apos;: 0, ...}">test_to_run = &apos;test_steps_harvest.py&apos;
testdir = &lt;Testdir local(&apos;/tmp/pytest-of-travis/pytest-0/test_run_all_tests3&apos;)&gt;

    @pytest.mark.parametrize(&apos;test_to_run&apos;, test_files, ids=str)
    def test_run_all_tests(test_to_run, testdir):
        &quot;&quot;&quot;
        This is a meta-test. It is executed for each test file in the &apos;tests_raw&apos; folder.
        For each of them, the file is retrieved and the expected test results are read from its first lines.
        Then a dedicated pytest runner is run on this file, and the results are compared with the expected ones.
    
        See https://docs.pytest.org/en/latest/writing_plugins.html
    
        :param test_to_run:
        :param testdir:
        :return:
        &quot;&quot;&quot;
    
        with open(join(tests_raw_folder, test_to_run)) as f:
            # Create a temporary conftest.py file
            # testdir.makeconftest(&quot;&quot;&quot;&quot;&quot;&quot;)
    
            # create a temporary pytest test file
            test_file_contents = f.read()
            testdir.makepyfile(test_file_contents)
    
            # Grab the expected things to check when this is executed
            m = META_REGEX.match(test_file_contents)
            if m is None:
                raise ValueError(&quot;test file &apos;%s&apos; does not contain the META-header&quot; % test_to_run)
            asserts_dct_str = m.groupdict()[&apos;asserts_dct&apos;]
            asserts_dct = ast.literal_eval(asserts_dct_str)
    
            # Here we run pytest
            print(&quot;\nTesting that running pytest on file %s results in %s&quot; % (test_to_run, str(asserts_dct)))
            result = testdir.runpytest()  # (&quot;-q&quot;)
    
            # Here we check that everything is ok
            try:
                result.assert_outcomes(**asserts_dct)
            except Exception as e:
                print(&quot;Error while asserting that %s results in %s&quot; % (test_to_run, str(asserts_dct)))
&gt;               six.raise_from(e, e)

/home/travis/build/smarie/python-pytest-steps/pytest_steps/tests/test_all.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
&lt;string&gt;:3: in raise_from
    ???
/home/travis/build/smarie/python-pytest-steps/pytest_steps/tests/test_all.py:57: in test_run_all_tests
    result.assert_outcomes(**asserts_dct)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_pytest.pytester.RunResult object at 0x7f2409a37a20&gt;, passed = 14
skipped = 0, failed = 0, error = 0, xpassed = 0, xfailed = 0

    def assert_outcomes(
        self, passed=0, skipped=0, failed=0, error=0, xpassed=0, xfailed=0
    ):
        &quot;&quot;&quot;Assert that the specified outcomes appear with the respective
        numbers (0 means it didn&apos;t occur) in the text output from a test run.
    
        &quot;&quot;&quot;
        d = self.parseoutcomes()
        obtained = {
            &quot;passed&quot;: d.get(&quot;passed&quot;, 0),
            &quot;skipped&quot;: d.get(&quot;skipped&quot;, 0),
            &quot;failed&quot;: d.get(&quot;failed&quot;, 0),
            &quot;error&quot;: d.get(&quot;error&quot;, 0),
            &quot;xpassed&quot;: d.get(&quot;xpassed&quot;, 0),
            &quot;xfailed&quot;: d.get(&quot;xfailed&quot;, 0),
        }
        expected = {
            &quot;passed&quot;: passed,
            &quot;skipped&quot;: skipped,
            &quot;failed&quot;: failed,
            &quot;error&quot;: error,
            &quot;xpassed&quot;: xpassed,
            &quot;xfailed&quot;: xfailed,
        }
&gt;       assert obtained == expected
E       AssertionError: assert {&apos;error&apos;: 0, &apos;failed&apos;: 1, &apos;passed&apos;: 13, &apos;skipped&apos;: 0, ...} == {&apos;error&apos;: 0, &apos;failed&apos;: 0, &apos;passed&apos;: 14, &apos;skipped&apos;: 0, ...}

/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/_pytest/pytester.py:444: AssertionError</failure><system-out>
Testing that running pytest on file test_steps_harvest.py results in {&apos;passed&apos;: 14, &apos;failed&apos;: 0, &apos;skipped&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests3, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.0, cov-2.6.0
collected 14 items

test_run_all_tests.py .F............                                     [100%]

=================================== FAILURES ===================================
________________________________ test_synthesis ________________________________

request = &lt;FixtureRequest for &lt;Function &apos;test_synthesis&apos;&gt;&gt;
store = OrderedDict()

    def test_synthesis(request, store):
        &quot;&quot;&quot;
        Note: we could do this at many other places (hook, teardown of a session-scope fixture...)
    
        Note2: we could provide helper methods in pytest_harvest to perform the code below more easily
        :param request:
        :param store:
        :return:
        &quot;&quot;&quot;
        # Get session synthesis
        # - filtered on the test function of interest
        # - combined with our store
        results_dct = get_session_synthesis_dct(request.session, filter=test_synthesis.__module__,
                                                durations_in_ms=True, test_id_format=&apos;function&apos;, status_details=False,
                                                fixture_store=store, flatten=True, flatten_more=&apos;my_results&apos;)
    
        # print keys and first node details
        assert len(results_dct) &gt; 0
        print(&quot;\nKeys:\n&quot; + &quot;\n&quot;.join(list(results_dct.keys())))
        print(&quot;\nFirst node:\n&quot; + &quot;\n&quot;.join(repr(k) + &quot;: &quot; + repr(v) for k, v in list(results_dct.values())[0].items()))
    
        # ---------- First version &quot;all by dataframe processing&quot; -----------
        param_names = {&apos;algo_param&apos;, &apos;dataset&apos;}
&gt;       tmp_df = build_df_from_raw_synthesis(results_dct, cross_steps_columns=param_names)

test_run_all_tests.py:114: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

results_dct = OrderedDict([(&apos;test_basic&apos;, OrderedDict([(&apos;pytest_obj&apos;, &lt;function test_basic at 0x7f23f5a161e0&gt;), (&apos;status&apos;, &apos;passed&apos;), (&apos;duration_ms&apos;, 0.23937225341796875)]))])
cross_steps_columns = {&apos;algo_param&apos;, &apos;dataset&apos;}

    def build_df_from_raw_synthesis(results_dct, cross_steps_columns):
        &quot;&quot;&quot;
        Converts the &apos;raw&apos; synthesis dct into a pivoted dataframe where steps are a level in multilevel columns
    
        :param results_dct:
        :return:
        &quot;&quot;&quot;
        # convert to a pandas dataframe
        results_df = pd.DataFrame.from_dict(results_dct, orient=&apos;index&apos;)  # this does not preserve rows order
        results_df = results_df.loc[list(results_dct.keys()), :]          # update rows order
    
        # (a) rename step id
        results_df.rename(columns={GENERATOR_MODE_STEP_ARGNAME: &apos;step_id&apos;}, inplace=True)
    
        # (b) create a column with the new id and use it as index in combination with step id
        # -- check column names provided
        non_present = set(cross_steps_columns) - set(results_df.columns)
        if len(non_present) &gt; 0:
            raise ValueError(&quot;Columns %s are not present in the resulting dataframe. Available columns: %s&quot;
&gt;                            &quot;&quot; % (non_present, list(results_df.columns)))
E           ValueError: Columns {&apos;dataset&apos;, &apos;algo_param&apos;} are not present in the resulting dataframe. Available columns: [&apos;pytest_obj&apos;, &apos;status&apos;, &apos;duration_ms&apos;]

test_run_all_tests.py:170: ValueError
----------------------------- Captured stdout call -----------------------------

Keys:
test_basic

First node:
&apos;pytest_obj&apos;: &lt;function test_basic at 0x7f23f5a161e0&gt;
&apos;status&apos;: &apos;passed&apos;
&apos;duration_ms&apos;: 0.23937225341796875
=============================== warnings summary ===============================
test_run_all_tests.py::test_synthesis
  /home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/pytest_harvest/results_session.py:341: UserWarning: [pytest-harvest] Test items status is not available. You should maybe install pytest-harvest with pip. If it is already the case, you case try to force-use it by adding `pytest_plugins = [&apos;harvest&apos;]` to your conftest.py. But for normal use this should not be required, installing with pip should be enough.
    warn(&quot;[pytest-harvest] Test items status is not available. You should maybe install pytest-harvest with &quot;

-- Docs: https://docs.pytest.org/en/latest/warnings.html
=============== 1 failed, 13 passed, 1 warnings in 0.50 seconds ================
Error while asserting that test_steps_harvest.py results in {&apos;passed&apos;: 14, &apos;failed&apos;: 0, &apos;skipped&apos;: 0}
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_steps_new_with_generator.py]" time="0.17721176147460938"><system-out>
Testing that running pytest on file test_steps_new_with_generator.py results in {&apos;passed&apos;: 12, &apos;failed&apos;: 2, &apos;skipped&apos;: 2}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests4, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.0, cov-2.6.0
collected 16 items

test_run_all_tests.py ......Fs.Fs.....                                   [100%]

=================================== FAILURES ===================================
________________ test_suite_exception_on_mandatory_step[step_b] ________________

________step_name_ = &apos;step_b&apos;
request = &lt;FixtureRequest for &lt;Function &apos;test_suite_exception_on_mandatory_step[step_b]&apos;&gt;&gt;

&gt;   ???

&lt;decorator-gen-7&gt;:2: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/travis/build/smarie/python-pytest-steps/pytest_steps/decorator_hack.py:154: in caller
    **kwargs)
/home/travis/build/smarie/python-pytest-steps/pytest_steps/steps_generator.py:431: in step_function_wrapper
    steps_monitor.execute(step_name, *args, **kwargs)
/home/travis/build/smarie/python-pytest-steps/pytest_steps/steps_generator.py:270: in execute
    res = next(self.gen)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @test_steps(&apos;step_a&apos;, &apos;step_b&apos;, &apos;step_c&apos;)
    def test_suite_exception_on_mandatory_step():
        &quot;&quot;&quot; &quot;&quot;&quot;
    
        # Step A
        print(&quot;step a&quot;)
        assert not False  # replace with your logic
        yield &apos;step_a&apos;
    
        # Step B
        print(&quot;step b&quot;)
&gt;       assert False  # replace with your logic
E       assert False

test_run_all_tests.py:43: AssertionError
----------------------------- Captured stdout call -----------------------------
step b
_______________ test_suite_optional_and_dependent_steps[step_b] ________________

________step_name_ = &apos;step_b&apos;
request = &lt;FixtureRequest for &lt;Function &apos;test_suite_optional_and_dependent_steps[step_b]&apos;&gt;&gt;

&gt;   ???

&lt;decorator-gen-8&gt;:2: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/travis/build/smarie/python-pytest-steps/pytest_steps/decorator_hack.py:154: in caller
    **kwargs)
/home/travis/build/smarie/python-pytest-steps/pytest_steps/steps_generator.py:431: in step_function_wrapper
    steps_monitor.execute(step_name, *args, **kwargs)
/home/travis/build/smarie/python-pytest-steps/pytest_steps/steps_generator.py:292: in execute
    raise six.reraise(res.exec_result.exc_type, res.exec_result.exc_val, res.exec_result.tb)
/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/six.py:693: in reraise
    raise value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @test_steps(&apos;step_a&apos;, &apos;step_b&apos;, &apos;step_c&apos;, &apos;step_d&apos;)
    def test_suite_optional_and_dependent_steps():
        &quot;&quot;&quot; &quot;&quot;&quot;
    
        # Step A
        print(&quot;step a&quot;)
        assert not False
        yield &apos;step_a&apos;
    
        # Step B
        with optional_step(&apos;step_b&apos;) as step_b:
            print(&quot;step b&quot;)
&gt;           assert False
E           assert False

test_run_all_tests.py:64: AssertionError
----------------------------- Captured stdout call -----------------------------
step b
================ 2 failed, 12 passed, 2 skipped in 0.13 seconds ================
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_pytest_capabilities.py]" time="0.10408353805541992"><system-out>
Testing that running pytest on file test_pytest_capabilities.py results in {&apos;passed&apos;: 16, &apos;failed&apos;: 0, &apos;skipped&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests5, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.0, cov-2.6.0
collected 16 items

test_run_all_tests.py ................                                   [100%]

========================== 16 passed in 0.06 seconds ===========================
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_steps_with_results_basic.py]" time="0.07311606407165527"><system-out>
Testing that running pytest on file test_steps_with_results_basic.py results in {&apos;passed&apos;: 4, &apos;failed&apos;: 0, &apos;skipped&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests6, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.0, cov-2.6.0
collected 4 items

test_run_all_tests.py ....                                               [100%]

=========================== 4 passed in 0.03 seconds ===========================
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_docs_example_with_harvest.py]" time="0.4080214500427246"><failure message="AssertionError: assert {&apos;error&apos;: 0, &apos;failed&apos;: 1, &apos;passed&apos;: 12, &apos;skipped&apos;: 0, ...} == {&apos;error&apos;: 0, &apos;failed&apos;: 0, &apos;passed&apos;: 13, &apos;skipped&apos;: 0, ...}">test_to_run = &apos;test_docs_example_with_harvest.py&apos;
testdir = &lt;Testdir local(&apos;/tmp/pytest-of-travis/pytest-0/test_run_all_tests7&apos;)&gt;

    @pytest.mark.parametrize(&apos;test_to_run&apos;, test_files, ids=str)
    def test_run_all_tests(test_to_run, testdir):
        &quot;&quot;&quot;
        This is a meta-test. It is executed for each test file in the &apos;tests_raw&apos; folder.
        For each of them, the file is retrieved and the expected test results are read from its first lines.
        Then a dedicated pytest runner is run on this file, and the results are compared with the expected ones.
    
        See https://docs.pytest.org/en/latest/writing_plugins.html
    
        :param test_to_run:
        :param testdir:
        :return:
        &quot;&quot;&quot;
    
        with open(join(tests_raw_folder, test_to_run)) as f:
            # Create a temporary conftest.py file
            # testdir.makeconftest(&quot;&quot;&quot;&quot;&quot;&quot;)
    
            # create a temporary pytest test file
            test_file_contents = f.read()
            testdir.makepyfile(test_file_contents)
    
            # Grab the expected things to check when this is executed
            m = META_REGEX.match(test_file_contents)
            if m is None:
                raise ValueError(&quot;test file &apos;%s&apos; does not contain the META-header&quot; % test_to_run)
            asserts_dct_str = m.groupdict()[&apos;asserts_dct&apos;]
            asserts_dct = ast.literal_eval(asserts_dct_str)
    
            # Here we run pytest
            print(&quot;\nTesting that running pytest on file %s results in %s&quot; % (test_to_run, str(asserts_dct)))
            result = testdir.runpytest()  # (&quot;-q&quot;)
    
            # Here we check that everything is ok
            try:
                result.assert_outcomes(**asserts_dct)
            except Exception as e:
                print(&quot;Error while asserting that %s results in %s&quot; % (test_to_run, str(asserts_dct)))
&gt;               six.raise_from(e, e)

/home/travis/build/smarie/python-pytest-steps/pytest_steps/tests/test_all.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
&lt;string&gt;:3: in raise_from
    ???
/home/travis/build/smarie/python-pytest-steps/pytest_steps/tests/test_all.py:57: in test_run_all_tests
    result.assert_outcomes(**asserts_dct)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_pytest.pytester.RunResult object at 0x7f23f5692f28&gt;, passed = 13
skipped = 0, failed = 0, error = 0, xpassed = 0, xfailed = 0

    def assert_outcomes(
        self, passed=0, skipped=0, failed=0, error=0, xpassed=0, xfailed=0
    ):
        &quot;&quot;&quot;Assert that the specified outcomes appear with the respective
        numbers (0 means it didn&apos;t occur) in the text output from a test run.
    
        &quot;&quot;&quot;
        d = self.parseoutcomes()
        obtained = {
            &quot;passed&quot;: d.get(&quot;passed&quot;, 0),
            &quot;skipped&quot;: d.get(&quot;skipped&quot;, 0),
            &quot;failed&quot;: d.get(&quot;failed&quot;, 0),
            &quot;error&quot;: d.get(&quot;error&quot;, 0),
            &quot;xpassed&quot;: d.get(&quot;xpassed&quot;, 0),
            &quot;xfailed&quot;: d.get(&quot;xfailed&quot;, 0),
        }
        expected = {
            &quot;passed&quot;: passed,
            &quot;skipped&quot;: skipped,
            &quot;failed&quot;: failed,
            &quot;error&quot;: error,
            &quot;xpassed&quot;: xpassed,
            &quot;xfailed&quot;: xfailed,
        }
&gt;       assert obtained == expected
E       AssertionError: assert {&apos;error&apos;: 0, &apos;failed&apos;: 1, &apos;passed&apos;: 12, &apos;skipped&apos;: 0, ...} == {&apos;error&apos;: 0, &apos;failed&apos;: 0, &apos;passed&apos;: 13, &apos;skipped&apos;: 0, ...}

/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/_pytest/pytester.py:444: AssertionError</failure><system-out>
Testing that running pytest on file test_docs_example_with_harvest.py results in {&apos;passed&apos;: 13, &apos;failed&apos;: 0, &apos;skipped&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests7, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.0, cov-2.6.0
collected 13 items

test_run_all_tests.py F............                                      [100%]

=================================== FAILURES ===================================
________________________________ test_synthesis ________________________________

request = &lt;FixtureRequest for &lt;Function &apos;test_synthesis&apos;&gt;&gt;
store = OrderedDict()

    def test_synthesis(request, store):
        &quot;&quot;&quot;
        Create the benchmark synthesis table.
        Note: we could do this at many other places (hook, teardown of a session-scope fixture...). See pytest-harvest
        &quot;&quot;&quot;
    
        # Get session synthesis using `pytest-harvest`
        # - filtered on the test function of interest
        # - combined with our store
        results_dct = get_session_synthesis_dct(request.session, filter=test_synthesis.__module__,
                                                durations_in_ms=True, test_id_format=&apos;function&apos;, status_details=False,
                                                fixture_store=store, flatten=True, flatten_more=&apos;my_results&apos;)
        # separate test id from step id when needed
        results_dct = handle_steps_in_synthesis_dct(results_dct, is_flat=True)
    
        # print keys and first node details
        print(&quot;\nKeys:\n&quot; + &quot;\n&quot;.join([str(t) for t in results_dct.keys()]))
&gt;       print(&quot;\nFirst node:\n&quot; + &quot;\n&quot;.join(repr(k) + &quot;: &quot; + repr(v) for k, v in list(results_dct.values())[0].items()))
E       IndexError: list index out of range

test_run_all_tests.py:89: IndexError
----------------------------- Captured stdout call -----------------------------

Keys:

=============================== warnings summary ===============================
test_run_all_tests.py::test_synthesis
  /home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/pytest_harvest/results_session.py:341: UserWarning: [pytest-harvest] Test items status is not available. You should maybe install pytest-harvest with pip. If it is already the case, you case try to force-use it by adding `pytest_plugins = [&apos;harvest&apos;]` to your conftest.py. But for normal use this should not be required, installing with pip should be enough.
    warn(&quot;[pytest-harvest] Test items status is not available. You should maybe install pytest-harvest with &quot;

-- Docs: https://docs.pytest.org/en/latest/warnings.html
=============== 1 failed, 12 passed, 1 warnings in 0.36 seconds ================
Error while asserting that test_docs_example_with_harvest.py results in {&apos;passed&apos;: 13, &apos;failed&apos;: 0, &apos;skipped&apos;: 0}
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_steps_no_results.py]" time="0.0699312686920166"><system-out>
Testing that running pytest on file test_steps_no_results.py results in {&apos;passed&apos;: 4, &apos;failed&apos;: 0, &apos;skipped&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests8, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.0, cov-2.6.0
collected 4 items

test_run_all_tests.py ....                                               [100%]

=========================== 4 passed in 0.02 seconds ===========================
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_decorator.py]" time="0.07573843002319336"><system-out>
Testing that running pytest on file test_decorator.py results in {&apos;passed&apos;: 4, &apos;failed&apos;: 0, &apos;skipped&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests9, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.0, cov-2.6.0
collected 4 items

test_run_all_tests.py ....                                               [100%]

=========================== 4 passed in 0.03 seconds ===========================
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_steps_dependencies.py]" time="0.09945225715637207"><system-out>
Testing that running pytest on file test_steps_dependencies.py results in {&apos;passed&apos;: 3, &apos;failed&apos;: 2, &apos;skipped&apos;: 1}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests10, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.0, cov-2.6.0
collected 6 items

test_run_all_tests.py .F..Fs                                             [100%]

=================================== FAILURES ===================================
_____________________________ test_suite_1[step_b] _____________________________

test_step = &apos;step_b&apos;

    @test_steps(&apos;step_a&apos;, &apos;step_b&apos;, &apos;step_c&apos;)
    def test_suite_1(test_step):
        &quot;&quot;&quot; In this test suite the last step can &quot;see&quot; the dependency so it is still executed ...&quot;&quot;&quot;
        # Execute the step according to name
        if test_step == &apos;step_a&apos;:
            step_a()
        elif test_step == &apos;step_b&apos;:
&gt;           step_b()

test_run_all_tests.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def step_b():
        &quot;&quot;&quot; Step a of the test &quot;&quot;&quot;
    
        # perform this step
        print(&quot;step b&quot;)
&gt;       assert False
E       assert False

test_run_all_tests.py:20: AssertionError
----------------------------- Captured stdout call -----------------------------
step b
________________________ test_suite_no_results[step_b] _________________________

test_step = &lt;function step_b at 0x7f23f4cda620&gt;
request = &lt;FixtureRequest for &lt;Function &apos;test_suite_no_results[step_b]&apos;&gt;&gt;

&gt;   ???

&lt;decorator-gen-19&gt;:2: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/travis/build/smarie/python-pytest-steps/pytest_steps/decorator_hack.py:154: in caller
    **kwargs)
/home/travis/build/smarie/python-pytest-steps/pytest_steps/steps_parametrizer.py:159: in dependency_mgr_wrapper
    res = test_func(*args, **kwargs)
test_run_all_tests.py:37: in test_suite_no_results
    test_step()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def step_b():
        &quot;&quot;&quot; Step a of the test &quot;&quot;&quot;
    
        # perform this step
        print(&quot;step b&quot;)
&gt;       assert False
E       assert False

test_run_all_tests.py:20: AssertionError
----------------------------- Captured stdout call -----------------------------
step b
================ 2 failed, 3 passed, 1 skipped in 0.05 seconds =================
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_stackoverflow2.py]" time="0.06630492210388184"><system-out>
Testing that running pytest on file test_stackoverflow2.py results in {&apos;passed&apos;: 3, &apos;failed&apos;: 0, &apos;skipped&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-3.10.1, py-1.7.0, pluggy-0.8.0
rootdir: /tmp/pytest-of-travis/pytest-0/test_run_all_tests11, inifile:
plugins: metadata-1.7.0, html-1.19.0, harvest-1.0.0, cov-2.6.0
collected 3 items

test_run_all_tests.py ...                                                [100%]

=========================== 3 passed in 0.02 seconds ===========================
</system-out></testcase></testsuite>