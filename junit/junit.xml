<?xml version="1.0" encoding="utf-8"?><testsuite errors="0" failures="1" name="pytest" skips="0" tests="10" time="0.813"><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_steps_with_results_complex.py]" time="0.07708954811096191"><system-out>
Testing that running pytest on file test_steps_with_results_complex.py results in {&apos;skipped&apos;: 0, &apos;passed&apos;: 16, &apos;failed&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.7.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests0, inifile: 
plugins: logging-2015.11.4, html-1.9.0, cov-2.6.0
collected 16 items

test_run_all_tests.py ................

========================== 16 passed in 0.04 seconds ===========================
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_wrapped_in_class.py]" time="0.06613516807556152"><failure message="assert 1 == 0
 +  where 0 = &lt;built-in method get of dict object at 0x7fe72224f048&gt;(&apos;passed&apos;, 0)
 +    where &lt;built-in method get of dict object at 0x7fe72224f048&gt; = {&apos;error&apos;: 1, &apos;seconds&apos;: 1}.get">test_to_run = &apos;test_wrapped_in_class.py&apos;
testdir = &lt;Testdir local(&apos;/tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests1&apos;)&gt;

    @pytest.mark.parametrize(&apos;test_to_run&apos;, test_files, ids=str)
    def test_run_all_tests(test_to_run, testdir):
        &quot;&quot;&quot;
        This is a meta-test. It is executed for each test file in the &apos;tests_raw&apos; folder.
        For each of them, the file is retrieved and the expected test results are read from its first lines.
        Then a dedicated pytest runner is run on this file, and the results are compared with the expected ones.
    
        See https://docs.pytest.org/en/latest/writing_plugins.html
    
        :param test_to_run:
        :param testdir:
        :return:
        &quot;&quot;&quot;
    
        with open(join(tests_raw_folder, test_to_run)) as f:
            # Create a temporary conftest.py file
            # testdir.makeconftest(&quot;&quot;&quot;&quot;&quot;&quot;)
    
            # create a temporary pytest test file
            test_file_contents = f.read()
            testdir.makepyfile(test_file_contents)
    
            # Grab the expected things to check when this is executed
            m = META_REGEX.match(test_file_contents)
            if m is None:
                raise ValueError(&quot;test file &apos;%s&apos; does not contain the META-header&quot; % test_to_run)
            asserts_dct_str = m.groupdict()[&apos;asserts_dct&apos;]
            asserts_dct = ast.literal_eval(asserts_dct_str)
    
            # Here we run pytest
            print(&quot;\nTesting that running pytest on file %s results in %s&quot; % (test_to_run, str(asserts_dct)))
            result = testdir.runpytest()  # (&quot;-q&quot;)
    
            # Here we check that everything is ok
            try:
                result.assert_outcomes(**asserts_dct)
            except Exception as e:
                print(&quot;Error while asserting that %s results in %s&quot; % (test_to_run, str(asserts_dct)))
&gt;               six.raise_from(e, e)

/home/travis/build/smarie/python-pytest-steps/pytest_steps/tests/test_all.py:60: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
&lt;string&gt;:3: in raise_from
    ???
/home/travis/build/smarie/python-pytest-steps/pytest_steps/tests/test_all.py:57: in test_run_all_tests
    result.assert_outcomes(**asserts_dct)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = &lt;_pytest.pytester.RunResult object at 0x7fe7222ad048&gt;, passed = 1
skipped = 0, failed = 0

    def assert_outcomes(self, passed=0, skipped=0, failed=0):
        &quot;&quot;&quot; assert that the specified outcomes appear with the respective
            numbers (0 means it didn&apos;t occur) in the text output from a test run.&quot;&quot;&quot;
        d = self.parseoutcomes()
&gt;       assert passed == d.get(&quot;passed&quot;, 0)
E       assert 1 == 0
E        +  where 0 = &lt;built-in method get of dict object at 0x7fe72224f048&gt;(&apos;passed&apos;, 0)
E        +    where &lt;built-in method get of dict object at 0x7fe72224f048&gt; = {&apos;error&apos;: 1, &apos;seconds&apos;: 1}.get

/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/_pytest/pytester.py:370: AssertionError</failure><system-out>
Testing that running pytest on file test_wrapped_in_class.py results in {&apos;skipped&apos;: 0, &apos;passed&apos;: 1, &apos;failed&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.7.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests1, inifile: 
plugins: logging-2015.11.4, html-1.9.0, cov-2.6.0
collected 0 items / 1 errors

==================================== ERRORS ====================================
____________________ ERROR collecting test_run_all_tests.py ____________________
test_run_all_tests.py:4: in &lt;module&gt;
    from pytest_harvest import get_session_synthesis_dct
E   ImportError: No module named &apos;pytest_harvest&apos;
=========================== 1 error in 0.01 seconds ============================
Error while asserting that test_wrapped_in_class.py results in {&apos;skipped&apos;: 0, &apos;passed&apos;: 1, &apos;failed&apos;: 0}
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_stackoverflow.py]" time="0.04240679740905762"><system-out>
Testing that running pytest on file test_stackoverflow.py results in {&apos;skipped&apos;: 0, &apos;passed&apos;: 3, &apos;failed&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.7.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests2, inifile: 
plugins: logging-2015.11.4, html-1.9.0, cov-2.6.0
collected 3 items

test_run_all_tests.py ...

=========================== 3 passed in 0.01 seconds ===========================
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_steps_new_with_generator.py]" time="0.15084195137023926"><system-out>
Testing that running pytest on file test_steps_new_with_generator.py results in {&apos;skipped&apos;: 2, &apos;passed&apos;: 12, &apos;failed&apos;: 2}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.7.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests3, inifile: 
plugins: logging-2015.11.4, html-1.9.0, cov-2.6.0
collected 16 items

test_run_all_tests.py ....Fs.Fs.......

=================================== FAILURES ===================================
________________ test_suite_exception_on_mandatory_step[step_b] ________________

________step_name_ = &apos;step_b&apos;
request = &lt;FixtureRequest for &lt;Function &apos;test_suite_exception_on_mandatory_step[step_b]&apos;&gt;&gt;

&gt;   ???

&lt;decorator-gen-4&gt;:2: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/travis/build/smarie/python-pytest-steps/pytest_steps/decorator_hack.py:154: in caller
    **kwargs)
/home/travis/build/smarie/python-pytest-steps/pytest_steps/steps_generator.py:431: in step_function_wrapper
    steps_monitor.execute(step_name, *args, **kwargs)
/home/travis/build/smarie/python-pytest-steps/pytest_steps/steps_generator.py:270: in execute
    res = next(self.gen)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @test_steps(&apos;step_a&apos;, &apos;step_b&apos;, &apos;step_c&apos;)
    def test_suite_exception_on_mandatory_step():
        &quot;&quot;&quot; &quot;&quot;&quot;
    
        # Step A
        print(&quot;step a&quot;)
        assert not False  # replace with your logic
        yield &apos;step_a&apos;
    
        # Step B
        print(&quot;step b&quot;)
&gt;       assert False  # replace with your logic
E       assert False

test_run_all_tests.py:43: AssertionError
----------------------------- Captured stdout call -----------------------------
step b
_______________ test_suite_optional_and_dependent_steps[step_b] ________________

________step_name_ = &apos;step_b&apos;
request = &lt;FixtureRequest for &lt;Function &apos;test_suite_optional_and_dependent_steps[step_b]&apos;&gt;&gt;

&gt;   ???

&lt;decorator-gen-5&gt;:2: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/travis/build/smarie/python-pytest-steps/pytest_steps/decorator_hack.py:154: in caller
    **kwargs)
/home/travis/build/smarie/python-pytest-steps/pytest_steps/steps_generator.py:431: in step_function_wrapper
    steps_monitor.execute(step_name, *args, **kwargs)
/home/travis/build/smarie/python-pytest-steps/pytest_steps/steps_generator.py:292: in execute
    raise six.reraise(res.exec_result.exc_type, res.exec_result.exc_val, res.exec_result.tb)
/home/travis/miniconda/envs/test-environment/lib/python3.5/site-packages/six.py:693: in reraise
    raise value
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    @test_steps(&apos;step_a&apos;, &apos;step_b&apos;, &apos;step_c&apos;, &apos;step_d&apos;)
    def test_suite_optional_and_dependent_steps():
        &quot;&quot;&quot; &quot;&quot;&quot;
    
        # Step A
        print(&quot;step a&quot;)
        assert not False
        yield &apos;step_a&apos;
    
        # Step B
        with optional_step(&apos;step_b&apos;) as step_b:
            print(&quot;step b&quot;)
&gt;           assert False
E           assert False

test_run_all_tests.py:64: AssertionError
----------------------------- Captured stdout call -----------------------------
step b
================ 2 failed, 12 passed, 2 skipped in 0.12 seconds ================
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_pytest_capabilities.py]" time="0.06626009941101074"><system-out>
Testing that running pytest on file test_pytest_capabilities.py results in {&apos;skipped&apos;: 0, &apos;passed&apos;: 16, &apos;failed&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.7.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests4, inifile: 
plugins: logging-2015.11.4, html-1.9.0, cov-2.6.0
collected 16 items

test_run_all_tests.py ................

========================== 16 passed in 0.04 seconds ===========================
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_steps_with_results_basic.py]" time="0.04560232162475586"><system-out>
Testing that running pytest on file test_steps_with_results_basic.py results in {&apos;skipped&apos;: 0, &apos;passed&apos;: 4, &apos;failed&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.7.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests5, inifile: 
plugins: logging-2015.11.4, html-1.9.0, cov-2.6.0
collected 4 items

test_run_all_tests.py ....

=========================== 4 passed in 0.02 seconds ===========================
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_steps_no_results.py]" time="0.043662309646606445"><system-out>
Testing that running pytest on file test_steps_no_results.py results in {&apos;skipped&apos;: 0, &apos;passed&apos;: 4, &apos;failed&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.7.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests6, inifile: 
plugins: logging-2015.11.4, html-1.9.0, cov-2.6.0
collected 4 items

test_run_all_tests.py ....

=========================== 4 passed in 0.01 seconds ===========================
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_decorator.py]" time="0.04671669006347656"><system-out>
Testing that running pytest on file test_decorator.py results in {&apos;skipped&apos;: 0, &apos;passed&apos;: 4, &apos;failed&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.7.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests7, inifile: 
plugins: logging-2015.11.4, html-1.9.0, cov-2.6.0
collected 4 items

test_run_all_tests.py ....

=========================== 4 passed in 0.02 seconds ===========================
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_steps_dependencies.py]" time="0.07135605812072754"><system-out>
Testing that running pytest on file test_steps_dependencies.py results in {&apos;skipped&apos;: 1, &apos;passed&apos;: 3, &apos;failed&apos;: 2}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.7.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests8, inifile: 
plugins: logging-2015.11.4, html-1.9.0, cov-2.6.0
collected 6 items

test_run_all_tests.py .F..Fs

=================================== FAILURES ===================================
_____________________________ test_suite_1[step_b] _____________________________

test_step = &apos;step_b&apos;

    @test_steps(&apos;step_a&apos;, &apos;step_b&apos;, &apos;step_c&apos;)
    def test_suite_1(test_step):
        &quot;&quot;&quot; In this test suite the last step can &quot;see&quot; the dependency so it is still executed ...&quot;&quot;&quot;
        # Execute the step according to name
        if test_step == &apos;step_a&apos;:
            step_a()
        elif test_step == &apos;step_b&apos;:
&gt;           step_b()

test_run_all_tests.py:47: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def step_b():
        &quot;&quot;&quot; Step a of the test &quot;&quot;&quot;
    
        # perform this step
        print(&quot;step b&quot;)
&gt;       assert False
E       assert False

test_run_all_tests.py:20: AssertionError
----------------------------- Captured stdout call -----------------------------
step b
________________________ test_suite_no_results[step_b] _________________________

test_step = &lt;function step_b at 0x7fe722065378&gt;
request = &lt;FixtureRequest for &lt;Function &apos;test_suite_no_results[step_b]&apos;&gt;&gt;

&gt;   ???

&lt;decorator-gen-13&gt;:2: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/home/travis/build/smarie/python-pytest-steps/pytest_steps/decorator_hack.py:154: in caller
    **kwargs)
/home/travis/build/smarie/python-pytest-steps/pytest_steps/steps_parametrizer.py:159: in dependency_mgr_wrapper
    res = test_func(*args, **kwargs)
test_run_all_tests.py:37: in test_suite_no_results
    test_step()
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def step_b():
        &quot;&quot;&quot; Step a of the test &quot;&quot;&quot;
    
        # perform this step
        print(&quot;step b&quot;)
&gt;       assert False
E       assert False

test_run_all_tests.py:20: AssertionError
----------------------------- Captured stdout call -----------------------------
step b
================ 2 failed, 3 passed, 1 skipped in 0.04 seconds =================
</system-out></testcase><testcase classname="pytest_steps.tests.test_all" file="pytest_steps/tests/test_all.py" line="21" name="test_run_all_tests[test_stackoverflow2.py]" time="0.04240870475769043"><system-out>
Testing that running pytest on file test_stackoverflow2.py results in {&apos;skipped&apos;: 0, &apos;passed&apos;: 3, &apos;failed&apos;: 0}
============================= test session starts ==============================
platform linux -- Python 3.5.6, pytest-2.9.2, py-1.7.0, pluggy-0.3.1
rootdir: /tmp/pytest-of-travis/pytest-0/testdir/test_run_all_tests9, inifile: 
plugins: logging-2015.11.4, html-1.9.0, cov-2.6.0
collected 3 items

test_run_all_tests.py ...

=========================== 3 passed in 0.01 seconds ===========================
</system-out></testcase></testsuite>